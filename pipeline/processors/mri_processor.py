"""
MRI Processor for hippocampal analysis pipeline.

This module orchestrates the complete MRI processing workflow,
from DICOM conversion through hippocampal asymmetry calculation.
"""

import json
import os
import platform
import re
import shutil
import subprocess as subprocess_module
import time
from pathlib import Path
from typing import Dict, List
from uuid import UUID

import nibabel as nib
import numpy as np
import pandas as pd
import requests

from backend.core.config import get_settings
from backend.core.logging import get_logger
from pipeline.utils import asymmetry, file_utils, segmentation, visualization

logger = get_logger(__name__)
settings = get_settings()

# FreeSurfer fallback constants
FREESURFER_CONTAINER_IMAGE = "freesurfer/freesurfer:7.4.1"  # Use direct FreeSurfer, not BIDS App
FREESURFER_CONTAINER_SIZE_GB = 20  # Updated for freesurfer/freesurfer:7.4.1
FREESURFER_PROCESSING_TIMEOUT_MINUTES = 180  # Extended for primary FreeSurfer usage
FREESURFER_DOWNLOAD_TIMEOUT_MINUTES = 20

# FreeSurfer Singularity constants (if available)
FREESURFER_SINGULARITY_IMAGE = None  # Will be determined dynamically
FREESURFER_SINGULARITY_SIZE_GB = 4

# FreeSurfer Native support removed - only container methods supported


class DockerNotAvailableError(Exception):
    """User-friendly exception when Docker is not available."""
    
    def __init__(self, error_type="not_installed"):
        self.error_type = error_type
        
        messages = {
            "not_installed": {
                "title": "Docker Desktop Not Installed",
                "message": "NeuroInsight requires Docker Desktop to process MRI scans.",
                "instructions": [
                    "1. Download Docker Desktop:",
                    "   ‚Ä¢ Windows/Mac: https://www.docker.com/get-started",
                    "   ‚Ä¢ Linux: https://docs.docker.com/engine/install/",
                    "",
                    "2. Install Docker Desktop (takes 10-15 minutes)",
                    "",
                    "3. Launch Docker Desktop and wait for the whale icon",
                    "",
                    "4. Return to NeuroInsight and try processing again"
                ],
                "why": "Docker is needed to run FreeSurfer, the brain segmentation tool."
            },
            "not_running": {
                "title": "Docker Desktop Not Running",
                "message": "Docker Desktop is installed but not currently running.",
                "instructions": [
                    "1. Open Docker Desktop from your Applications folder",
                    "",
                    "2. Wait for the whale icon to appear in your system tray:",
                    "   ‚Ä¢ macOS: Top menu bar",
                    "   ‚Ä¢ Windows: System tray (bottom right)",
                    "   ‚Ä¢ Linux: System tray",
                    "",
                    "3. The icon should be steady (not animating)",
                    "",
                    "4. Return to NeuroInsight and try processing again"
                ],
                "why": "Docker must be running to process MRI scans."
            },
            "image_not_found": {
                "title": "Downloading Brain Segmentation Model",
                "message": "First-time setup: Downloading FreeSurfer (~4GB).",
                "instructions": [
                    "This download happens only once and takes 10-15 minutes.",
                    "",
                    "The model will be cached for future use.",
                    "",
                    "Please keep Docker Desktop running and wait..."
                ],
                "why": "NeuroInsight needs to download the brain segmentation AI model."
            }
        }
        
        error_info = messages.get(error_type, messages["not_installed"])
        
        # Format the error message
        full_message = f"\n{'='*60}\n"
        full_message += f"{error_info['title']}\n"
        full_message += f"{'='*60}\n\n"
        full_message += f"{error_info['message']}\n\n"
        full_message += "What to do:\n"
        full_message += "\n".join(error_info['instructions'])
        full_message += f"\n\nWhy: {error_info['why']}\n"
        full_message += f"{'='*60}\n"
        
        super().__init__(full_message)
        self.title = error_info['title']
        self.user_message = error_info['message']
        self.instructions = error_info['instructions']


        self.title = error_info['title']
        self.user_message = error_info['message']
        self.instructions = error_info['instructions']


class MRIProcessor:
    """
    Main processor for MRI hippocampal analysis.
    
    Orchestrates the complete pipeline:
    1. File format validation/conversion
    2. FreeSurfer segmentation
    3. Hippocampal subfield extraction
    4. Volumetric analysis
    5. Asymmetry index calculation
    """
    
    def __init__(self, job_id: UUID, progress_callback=None, db_session=None):
        """
        Initialize MRI processor.

        Args:
            job_id: Unique job identifier
            progress_callback: Optional callback function(progress: int, step: str) for progress updates
            db_session: Optional database session for progress persistence
        """
        print(f"DEBUG: MRIProcessor.__init__ called with job_id={job_id}")
        self.job_id = job_id
        self.db_session = db_session
        self.app_dir = Path(__file__).parent.parent.parent.absolute()  # Path to project root
        self.output_dir = Path(settings.output_dir) / str(job_id)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.process_pid = None  # Track subprocess PID for cleanup
        self.progress_callback = progress_callback

        # Check if smoke test mode is enabled (for CI/testing)
        self.smoke_test_mode = os.getenv("FASTSURFER_SMOKE_TEST") == "1"

        # Initialize progress tracking
        self._current_progress = 0

        logger.info(
            "processor_initialized",
            job_id=str(job_id)
        )

    def _get_current_progress(self) -> int:
        """
        Get current processing progress percentage.

        Returns:
            Current progress as integer percentage (0-100)
        """
        return getattr(self, '_current_progress', 0)

    def _update_progress(self, progress: int, step: str):
        """
        Update current progress and notify callback if available.
        Also persist progress to database in worker context.

        Args:
            progress: Progress percentage (0-100)
            step: Current processing step description
        """
        self._current_progress = progress

        # Update database if we're in a worker context (has db_session)
        if hasattr(self, 'db_session') and self.db_session:
            try:
                from workers.tasks.processing_web import update_job_progress
                update_job_progress(self.db_session, self.job_id, progress, step)
            except Exception as e:
                logger.warning("failed_to_update_job_progress_in_db", error=str(e), progress=progress, step=step)

        # Notify callback if available (for Celery task state)
        if self.progress_callback:
            self.progress_callback(progress, step)

    def _store_container_id(self, container_id: str):
        """
        Store Docker container ID in database for cancellation support.
        
        Args:
            container_id: Docker container ID or name
        """
        if hasattr(self, 'db_session') and self.db_session:
            try:
                from sqlalchemy import update
                from backend.models.job import Job
                
                self.db_session.execute(
                    update(Job)
                    .where(Job.id == str(self.job_id))
                    .values(docker_container_id=container_id)
                )
                self.db_session.commit()
                logger.info("stored_container_id", job_id=str(self.job_id), container_id=container_id)
            except Exception as e:
                logger.warning("failed_to_store_container_id", error=str(e), container_id=container_id)
                self.db_session.rollback()

    def _cleanup_job_containers(self) -> None:
        """
        Clean up any running Docker containers for this job.

        This ensures containers don't remain running after job completion,
        preventing concurrency limit exhaustion.
        """
        try:
            container_name = f"freesurfer-job-{self.job_id}"
            logger.info("cleaning_up_job_containers", job_id=str(self.job_id), container_name=container_name)

            # Stop the container if it's running
            result = subprocess_module.run(
                ["docker", "stop", container_name],
                capture_output=True,
                timeout=30
            )

            if result.returncode == 0:
                logger.info("stopped_job_container", job_id=str(self.job_id), container_name=container_name)
            else:
                # Container might not be running, which is fine
                logger.debug("container_already_stopped_or_not_found",
                           job_id=str(self.job_id),
                           container_name=container_name,
                           stderr=result.stderr.decode() if result.stderr else "")

        except subprocess_module.TimeoutExpired:
            logger.warning("container_stop_timeout", job_id=str(self.job_id), container_name=container_name)
        except Exception as e:
            logger.warning("container_cleanup_failed", job_id=str(self.job_id), error=str(e))

    def validate_disk_space(self) -> None:
        """
        Validate that sufficient disk space is available for processing.

        Raises:
            OSError: If disk space is insufficient for processing
        """
        try:
            # Check disk space in the upload directory (where processing happens)
            working_dir = Path(settings.upload_dir).parent
            disk_stats = shutil.disk_usage(working_dir)

            # Available space in GB
            available_gb = disk_stats.free / (1024**3)
            total_gb = disk_stats.total / (1024**3)

            logger.info(
                "disk_space_check",
                available_gb=round(available_gb, 2),
                total_gb=round(total_gb, 2)
            )

            # Require at least 10GB free space for processing
            MIN_REQUIRED_GB = 10.0

            if available_gb < MIN_REQUIRED_GB:
                error_msg = (
                    f"Insufficient disk space for MRI processing.\n"
                    f"Available: {available_gb:.1f} GB\n"
                    f"Required: {MIN_REQUIRED_GB:.1f} GB minimum\n\n"
                    f"Processing MRI scans requires temporary storage for:\n"
                    f"‚Ä¢ Docker container images (~4GB)\n"
                    f"‚Ä¢ Intermediate processing files\n"
                    f"‚Ä¢ Output visualizations and reports\n\n"
                    f"Please free up disk space and try again."
                )
                logger.error(
                    "insufficient_disk_space",
                    available_gb=available_gb,
                    required_gb=MIN_REQUIRED_GB
                )
                raise OSError(error_msg)

            # Warn if space is getting low (less than 20GB)
            if available_gb < 20.0:
                logger.warning(
                    "disk_space_low_warning",
                    available_gb=available_gb,
                    message="Disk space is getting low. Consider freeing up space for better performance."
                )

        except Exception as e:
            logger.warning("disk_space_check_failed", error=str(e))
            # Don't fail processing if we can't check disk space
            # Just log the warning

    def validate_memory(self) -> None:
        """
        Validate that sufficient RAM is available for processing.

        Raises:
            MemoryError: If system memory is insufficient
        """
        try:
            import psutil
        except ImportError:
            logger.warning("psutil_not_available_memory_check_skipped")
            return

        try:
            # Get system memory info
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            total_gb = memory.total / (1024**3)

            logger.info(
                "memory_check",
                available_gb=round(available_gb, 2),
                total_gb=round(total_gb, 2)
            )

            # Memory recommendations for different use cases
            INSTALL_MIN_GB = 7.0    # Allows installation
            PROCESS_MIN_GB = 16.0   # Reliable processing
            RECOMMENDED_GB = 32.0   # Optimal performance

            # Warnings for different memory levels
            if total_gb < PROCESS_MIN_GB:
                warning_msg = (
                    f"‚ö†Ô∏è  LIMITED MEMORY WARNING\n\n"
                    f"System RAM: {total_gb:.1f} GB\n"
                    f"Recommended for processing: {PROCESS_MIN_GB:.1f} GB+\n\n"
                    f"With {total_gb:.1f} GB RAM, MRI processing may:\n"
                    f"‚Ä¢ Fail due to insufficient memory\n"
                    f"‚Ä¢ Take significantly longer\n"
                    f"‚Ä¢ Cause system slowdowns\n\n"
                    f"For reliable MRI processing, consider upgrading to 16GB+ RAM.\n"
                    f"FreeSurfer segmentation typically requires 4-8GB per brain.\n\n"
                    f"Continuing with processing, but failures are likely..."
                )
                logger.warning(
                    "low_memory_warning",
                    total_gb=total_gb,
                    recommended_gb=PROCESS_MIN_GB,
                    message="Processing may fail due to insufficient RAM"
                )
                print(f"\n{warning_msg}\n")
                # Don't raise error - allow processing attempt but warn heavily

            elif total_gb < RECOMMENDED_GB:
                info_msg = (
                    f"‚ÑπÔ∏è  MEMORY INFO\n"
                    f"System has {total_gb:.1f} GB RAM - sufficient for basic processing.\n"
                    f"For optimal performance with multiple jobs, consider 32GB+ RAM."
                )
                logger.info(
                    "adequate_memory",
                    total_gb=total_gb,
                    recommended_gb=RECOMMENDED_GB
                )
                print(f"\n{info_msg}\n")

            else:
                logger.info(
                    "optimal_memory",
                    total_gb=total_gb,
                    message="System has optimal RAM for NeuroInsight processing"
                )

            # Warn if memory is getting low
            if available_gb < 4.0:
                logger.warning(
                    "memory_low_warning",
                    available_gb=available_gb,
                    message="Available memory is low. Processing may be slow or fail."
                )

        except Exception as e:
            logger.warning("memory_check_failed", error=str(e))
            # Don't fail processing if we can't check memory
            # Just log the warning

    def validate_network_connectivity(self) -> None:
        """
        Validate network connectivity for Docker image downloads.

        Warns if network issues are detected but doesn't fail processing.
        """
        try:
            import urllib.request
            import socket

            # Set timeout for network checks
            socket.setdefaulttimeout(10)

            # Test connection to Docker Hub
            test_urls = [
                "https://registry-1.docker.io",  # Docker Hub
                "https://hub.docker.com",        # Docker Hub website
            ]

            network_ok = False
            for url in test_urls:
                try:
                    logger.info("testing_network_connectivity", url=url)
                    req = urllib.request.Request(url, method='HEAD')
                    with urllib.request.urlopen(req, timeout=10) as response:
                        if response.status == 200:
                            network_ok = True
                            break
                except Exception as e:
                    logger.debug(f"network_test_failed_for_{url}", error=str(e))
                    continue

            if not network_ok:
                logger.warning(
                    "network_connectivity_issues",
                    message="Unable to reach Docker Hub. Image downloads may fail or be slow.",
                    suggestion="Check your internet connection and firewall settings."
                )
            else:
                logger.info("network_connectivity_ok")

        except Exception as e:
            logger.warning("network_check_failed", error=str(e))
            # Don't fail processing if we can't check network

    def _download_docker_image_with_progress(self, image_name: str, display_name: str, env: Dict = None) -> None:
        """
        Download Docker image with enhanced progress messages and error handling.

        Args:
            image_name: Full Docker image name (e.g., 'deepmi/fastsurfer:latest')
            display_name: Human-readable name for progress messages
            env: Environment variables for Docker command

        Raises:
            subprocess_module.CalledProcessError: If Docker pull fails
        """
        if self.progress_callback:
            self.progress_callback(
                self._get_current_progress(),
                f"Downloading {display_name} Docker image (~4GB). This may take 10-15 minutes..."
            )

        logger.info(f"starting_download_{image_name.replace('/', '_')}")

        if env is None:
            env = self._get_extended_env()

        env.update({
            'DOCKER_CLI_HINTS': 'false',
            'DOCKER_HIDE_LEGACY_COMMANDS': 'true',
            'DOCKER_CLI_EXPERIMENTAL': 'disabled'
        })

        try:
            # First, try to pull without quiet flag to show progress (if supported)
            # Fall back to quiet mode if progress display fails
            try:
                logger.info("attempting_docker_pull_with_progress")
                result = subprocess_module.run(
                    ["docker", "pull", image_name],
                    capture_output=False,  # Let user see progress
                    timeout=FREESURFER_DOWNLOAD_TIMEOUT_MINUTES*60,
                    env=env
                )
            except subprocess_module.TimeoutExpired:
                raise subprocess_module.TimeoutExpired(
                    cmd=["docker", "pull", image_name],
                    timeout=FREESURFER_DOWNLOAD_TIMEOUT_MINUTES*60,
                    output=None,
                    stderr=f"Docker image download timed out after {FREESURFER_DOWNLOAD_TIMEOUT_MINUTES} minutes"
                )

            if result.returncode == 0:
                logger.info(f"{image_name.replace('/', '_')}_download_successful")
                if self.progress_callback:
                    self.progress_callback(
                        self._get_current_progress(),
                        f" {display_name} ready - continuing processing..."
                    )
            else:
                error_msg = result.stderr.decode() if result.stderr else "Unknown error"

                # Enhanced error messages for common Docker issues
                if "timeout" in error_msg.lower():
                    error_msg = f"Docker image download timed out. This can happen with slow internet connections.\n\nTroubleshooting:\n‚Ä¢ Check your internet speed\n‚Ä¢ Try again later when network is faster\n‚Ä¢ Use a wired connection if on WiFi\n‚Ä¢ Original error: {error_msg}"
                elif "no space left on device" in error_msg.lower():
                    error_msg = f"Insufficient disk space for Docker image download.\n\nThe {display_name} image requires ~4GB of free space.\n\nTroubleshooting:\n‚Ä¢ Free up at least 5GB of disk space\n‚Ä¢ Run 'docker system prune' to clean up old images\n‚Ä¢ Check available space with 'df -h'\n‚Ä¢ Original error: {error_msg}"
                elif "network" in error_msg.lower() or "connection" in error_msg.lower():
                    error_msg = f"Network connectivity issue during Docker download.\n\nTroubleshooting:\n‚Ä¢ Check your internet connection\n‚Ä¢ Try disabling VPN if active\n‚Ä¢ Check firewall/proxy settings\n‚Ä¢ Try 'docker pull hello-world' to test basic connectivity\n‚Ä¢ Original error: {error_msg}"
                elif "denied" in error_msg.lower() or "unauthorized" in error_msg.lower():
                    error_msg = f"Docker registry access denied.\n\nTroubleshooting:\n‚Ä¢ Ensure you're logged into Docker Hub if needed\n‚Ä¢ Check if you're behind a corporate firewall\n‚Ä¢ Try 'docker login' if you have Docker Hub credentials\n‚Ä¢ Original error: {error_msg}"

                logger.error(f"{image_name.replace('/', '_')}_download_failed", error=error_msg)
                raise subprocess_module.CalledProcessError(
                    result.returncode,
                    ["docker", "pull", image_name],
                    None,
                    error_msg
                )

        except subprocess_module.TimeoutExpired as e:
            timeout_msg = f"Docker image download for {display_name} timed out after {FREESURFER_DOWNLOAD_TIMEOUT_MINUTES} minutes.\n\nThis usually happens with slow internet connections. Try:\n‚Ä¢ Using a faster internet connection\n‚Ä¢ Downloading during off-peak hours\n‚Ä¢ Using a wired connection instead of WiFi\n‚Ä¢ Contacting your network administrator if on a corporate network"
            logger.error("docker_download_timeout", image=image_name, timeout_minutes=FREESURFER_DOWNLOAD_TIMEOUT_MINUTES)
            raise subprocess_module.TimeoutExpired(e.cmd, e.timeout, e.output, timeout_msg)

        except subprocess_module.CalledProcessError:
            # Re-raise with enhanced error message
            raise

    def process(self, input_path: str) -> Dict:
        """
        Execute the complete processing pipeline.

        Args:
            input_path: Path to input MRI file (DICOM or NIfTI)

        Returns:
            Dictionary containing processing results and metrics
        """
        logger.info("processing_pipeline_started", job_id=str(self.job_id))
        print(f"DEBUG: MRI Processor process() called for job {self.job_id}")

        # Check if we should use real FreeSurfer processing via API bridge
        use_real_freesurfer = os.getenv("USE_REAL_FREESURFER", "false").lower() == "true"
        use_mock = os.getenv("USE_MOCK_PROCESSING", "false").lower() == "true"

        if use_real_freesurfer:
            return self._api_bridge_process(input_path)
        elif use_mock:
            return self._mock_process(input_path)

        # Validate system requirements
        self.validate_disk_space()
        self.validate_memory()
        self.validate_network_connectivity()

        # Step 1: Convert to NIfTI if needed
        self._update_progress(17, "Preparing input file...")
        nifti_path = self._prepare_input(input_path)
        
        # Step 2: Run FreeSurfer segmentation (whole brain) - LONGEST STEP
        self._update_progress(20, "Running complete FreeSurfer segmentation (autorecon1 + autorecon2-volonly + mri_segstats)...")
        freesurfer_output = self._run_freesurfer_primary(nifti_path)
        
        # Step 3: Extract hippocampal volumes (from FreeSurfer outputs)
        self._update_progress(65, "Extracting hippocampal volumes...")
        logger.info("extracting_hippocampal_data_from_freesurfer_output", freesurfer_output=str(freesurfer_output))
        hippocampal_stats = self._extract_hippocampal_data(freesurfer_output)
        logger.info("hippocampal_stats_extracted", stats=hippocampal_stats)

        # Validate that we have real hippocampal data
        if not hippocampal_stats:
            error_msg = ("Failed to extract hippocampal volume data from FreeSurfer output. "
                        "FreeSurfer processing may have failed or produced incomplete results. "
                        f"Check FreeSurfer logs in {freesurfer_output}")

            # Check if mock data fallback is allowed (development/testing only)
            allow_mock_fallback = os.getenv("NEUROINSIGHT_ALLOW_MOCK_FALLBACK", "false").lower() == "true"

            if allow_mock_fallback:
                logger.warning("no_hippocampal_data_extracted_creating_mock_fallback",
                             error=error_msg,
                             freesurfer_output=str(freesurfer_output))
                # Create mock data for development/testing only
                self._create_mock_freesurfer_output(freesurfer_output)
                hippocampal_stats = self._extract_hippocampal_data(freesurfer_output)
                logger.info("mock_hippocampal_data_created_for_development", stats=hippocampal_stats)
            else:
                # Production: Fail with clear error message
                logger.error("hippocampal_extraction_failed_no_fallback",
                           error=error_msg,
                           freesurfer_output=str(freesurfer_output))
                raise RuntimeError(f"Hippocampal segmentation failed: {error_msg}")

        # Step 4: Calculate asymmetry indices
        self._update_progress(70, "Calculating asymmetry indices...")
        logger.info("calculating_asymmetry_from_stats", hippocampal_stats=hippocampal_stats)
        metrics = self._calculate_asymmetry(hippocampal_stats)
        logger.info("asymmetry_metrics_calculated", metrics=metrics, metrics_count=len(metrics))
        
        # Step 5: Generate segmentation visualizations
        self._update_progress(75, "Generating visualizations...")
        visualization_paths = self._generate_visualizations(nifti_path, freesurfer_output)
        
        # Step 6: Save results
        self._update_progress(82, "Saving results...")
        self._save_results(metrics)
        
        logger.info(
            "processing_pipeline_completed",
            job_id=str(self.job_id),
            metrics_count=len(metrics),
        )

        # Ensure any orphaned containers for this job are cleaned up
        self._cleanup_job_containers()

        return {
            "job_id": str(self.job_id),
            "output_dir": str(self.output_dir),
            "metrics": metrics,
            "visualizations": visualization_paths,
        }
    
    def _prepare_input(self, input_path: str) -> Path:
        """
        Prepare input file for processing.
        
        Converts DICOM to NIfTI if needed, validates format.
        
        Args:
            input_path: Path to input file
        
        Returns:
            Path to NIfTI file
        """
        input_file = Path(input_path)
        
        # If already NIfTI, validate and return
        if input_file.suffix in [".nii", ".gz"]:
            if file_utils.validate_nifti(input_file):
                logger.info("input_validated", format="NIfTI")
                return input_file
            else:
                raise ValueError(f"NIfTI file validation failed: {input_file}")
        
        # Convert DICOM to NIfTI
        elif input_file.suffix in [".dcm", ".dicom"]:
            logger.info("converting_dicom_to_nifti")
            output_path = self.output_dir / "input.nii.gz"
            file_utils.convert_dicom_to_nifti(input_file, output_path)
            return output_path
        
        else:
            raise ValueError(f"Unsupported file format: {input_file.suffix}")
    
    def _is_docker_available(self) -> bool:
        """
        Quick check if Docker is available (for fallback logic).
        """
        try:
            result = subprocess_module.run(
                ["docker", "version"],
                capture_output=True,
                timeout=5,
                env=self._get_extended_env()
            )
            return result.returncode == 0
        except:
            return False

    def _is_singularity_available(self) -> bool:
        """
        Quick check if Singularity/Apptainer is available (for fallback logic).
        """
        for cmd in ["apptainer", "singularity"]:
            try:
                result = subprocess_module.run(
                    [cmd, "--version"],
                    capture_output=True,
                    timeout=5,
                    env=self._get_extended_env()
                )
                if result.returncode == 0:
                    return True
            except:
                continue
        return False

    # _is_native_freesurfer_available method removed - native FreeSurfer support disabled

    def _get_extended_env(self) -> dict:
        """
        Get environment with extended PATH for container runtimes.
        """
        env = os.environ.copy()
        current_path = env.get('PATH', '')
        user_home = os.path.expanduser('~')

        # Add common container runtime locations
        extra_paths = [
            f'{user_home}/bin',
            '/usr/local/bin',
            '/usr/bin',
            '/bin',
            '/opt/bin',
            '/snap/bin',
            '/opt/singularity/bin',
            '/opt/apptainer/bin',
        ]

        extended_path = current_path
        for path in extra_paths:
            if path not in current_path:
                extended_path = f"{path}:{extended_path}"

        env['PATH'] = extended_path
        return env

    def _check_container_runtime_availability(self) -> str:
        """
        Check which FreeSurfer container execution method is available and preferred.

        Returns:
            "docker", "singularity", or "none"
        """
        import shutil

        logger.info("checking_freesurfer_container_runtimes", note="Starting FreeSurfer container runtime detection")

        # Check container FreeSurfer execution methods only (no native support)
        docker_available = False
        singularity_available = False
        singularity_cmd = None

        # Check which command is available with extended PATH
        # Similar to Docker detection, extend PATH for common Singularity locations
        env = os.environ.copy()
        current_path = env.get('PATH', '')
        logger.info("checking_singularity_path", path=current_path)

        # Add common Singularity/Apptainer locations to PATH
        import getpass
        user_home = os.path.expanduser('~')
        singularity_paths = [
            f'{user_home}/bin',           # User's bin directory
            '/usr/local/bin',             # Manual installs
            '/usr/bin',                   # System default
            '/bin',                       # Fallback system path
            '/opt/bin',                   # Optional packages
            '/opt/singularity/bin',       # Singularity default install
            '/opt/apptainer/bin',         # Apptainer default install
            '/usr/local/singularity/bin', # Alternative Singularity location
            '/usr/local/apptainer/bin',   # Alternative Apptainer location
            '/opt/modulefiles/bin',       # Module system locations
            '/cm/shared/apps/singularity', # Common HPC locations
            '/shared/apps/singularity',
            '/opt/apps/singularity',
            '/opt/hpc/singularity',
        ]

        # Add configured Singularity bin path if specified
        if hasattr(settings, 'singularity_bin_path') and settings.singularity_bin_path:
            singularity_paths.insert(0, settings.singularity_bin_path)
            logger.info("added_configured_singularity_path", path=settings.singularity_bin_path)

        extended_path = current_path
        for path in singularity_paths:
            if path not in current_path:
                extended_path = f"{path}:{extended_path}"
        env['PATH'] = extended_path

        logger.info("extended_singularity_path", path=extended_path)

        # Check for commands with extended PATH
        def check_command_with_path(cmd):
            """Check if command exists using extended PATH"""
            try:
                result = subprocess_module.run(
                    ['which', cmd],
                    capture_output=True,
                    text=True,
                    env=env,
                    timeout=5
                )
                if result.returncode == 0 and result.stdout.strip():
                    logger.info("found_command_with_extended_path", command=cmd, path=result.stdout.strip())
                    return True
            except (subprocess_module.TimeoutExpired, subprocess_module.CalledProcessError):
                pass
            return False

        # Check for Apptainer first (newer, more actively maintained)
        if check_command_with_path("apptainer"):
            singularity_cmd = "apptainer"
            logger.info("found_apptainer_command")
        elif check_command_with_path("singularity"):
            singularity_cmd = "singularity"
            logger.info("found_singularity_command")
        else:
            # Fallback: check absolute paths directly if PATH-based checks fail
            logger.info("path_based_checks_failed_trying_absolute_paths")
            if os.path.exists('/usr/bin/apptainer') and os.access('/usr/bin/apptainer', os.X_OK):
                singularity_cmd = "/usr/bin/apptainer"
                logger.info("found_apptainer_via_absolute_path", path="/usr/bin/apptainer")
            elif os.path.exists('/usr/bin/singularity') and os.access('/usr/bin/singularity', os.X_OK):
                singularity_cmd = "/usr/bin/singularity"
                logger.info("found_singularity_via_absolute_path", path="/usr/bin/singularity")
            elif os.path.exists('/usr/local/bin/apptainer') and os.access('/usr/local/bin/apptainer', os.X_OK):
                singularity_cmd = "/usr/local/bin/apptainer"
                logger.info("found_apptainer_via_absolute_path", path="/usr/local/bin/apptainer")
            else:
                logger.warning("no_singularity_commands_found")

        if singularity_cmd:
            # Quick test to see if Singularity works with extended PATH
            try:
                logger.info("testing_singularity_version", command=singularity_cmd)
                result = subprocess_module.run(
                    [singularity_cmd, "--version"],
                    capture_output=True,
                    timeout=5,
                    env=env
                )
                if result.returncode == 0:
                    singularity_available = True
                    logger.info("singularity_available", version=result.stdout.decode().strip() if result.stdout else "unknown")
                else:
                    logger.warning("singularity_version_check_failed", returncode=result.returncode, stderr=result.stderr.decode()[:100] if result.stderr else "no stderr")
            except (FileNotFoundError, subprocess_module.TimeoutExpired, subprocess_module.CalledProcessError) as e:
                logger.warning("singularity_test_failed", error=str(e))

        # Check for Docker
        docker_available = False
        try:
            # First try with explicit PATH that includes common Docker locations
            env = os.environ.copy()
            current_path = env.get('PATH', '')
            logger.info("current_path", path=current_path)

            # Add common Docker locations to PATH
            import getpass
            user_home = os.path.expanduser('~')
            docker_paths = [
                f'{user_home}/bin',       # User's bin directory (most common)
                '/usr/local/bin',         # Manual installs, Homebrew (Linux)
                '/usr/bin',               # System default (apt, dnf, pacman, etc.)
                '/bin',                   # Fallback system path
                '/opt/bin',               # Optional packages
                '/snap/bin',              # Snap packages
                '/opt/docker-desktop/bin', # Docker Desktop for Linux
                '/opt/docker/bin',        # Alternative Docker installs
            ]
            extended_path = current_path
            for path in docker_paths:
                if path not in current_path:
                    extended_path = f"{path}:{extended_path}"
            env['PATH'] = extended_path

            logger.info("extended_path", path=extended_path)
            logger.info("testing_docker_availability")

            # Try with extended PATH first
            result = subprocess_module.run(
                ["docker", "version"],
                capture_output=True,
                timeout=5,
                env=env
            )
            if result.returncode == 0:
                docker_available = True
                logger.info("docker_available", version=result.stdout.decode().strip()[:50] if result.stdout else "unknown")
            else:
                logger.warning("docker_version_check_failed", returncode=result.returncode, stderr=result.stderr.decode()[:100] if result.stderr else "no stderr")

                # Fallback: try absolute paths directly
                logger.info("docker_path_check_failed_trying_absolute_paths")
                docker_binary_paths = [
                    f"{user_home}/bin/docker",
                    "/usr/bin/docker",
                    "/usr/local/bin/docker",
                    "/opt/docker/bin/docker",
                    "/opt/docker-desktop/bin/docker",
                    "/snap/bin/docker"
                ]

                for docker_path in docker_binary_paths:
                    if os.path.exists(docker_path) and os.access(docker_path, os.X_OK):
                        logger.info("found_docker_via_absolute_path", path=docker_path)
                        try:
                            result = subprocess_module.run(
                                [docker_path, "version"],
                                capture_output=True,
                                timeout=5
                            )
                            if result.returncode == 0:
                                docker_available = True
                                logger.info("docker_available_via_absolute_path", path=docker_path, version=result.stdout.decode().strip()[:50] if result.stdout else "unknown")
                                break
                        except (subprocess_module.TimeoutExpired, subprocess_module.CalledProcessError, FileNotFoundError) as e:
                            logger.debug("docker_test_failed_at_path", path=docker_path, error=str(e))
                            continue

        except (FileNotFoundError, subprocess_module.TimeoutExpired, subprocess_module.CalledProcessError) as e:
            logger.warning("docker_test_failed", error=str(e))

        # FreeSurfer Runtime Selection Logic:
        # 1. Singularity is preferred for HPC environments and stability
        # 2. Docker is used as fallback for broader compatibility
        # 3. Mock data is used only when no container method works

        prefer_singularity = getattr(settings, 'prefer_singularity', True)  # Force Singularity due to Docker issues

        logger.info("freesurfer_runtime_selection_starting",
                   docker_available=docker_available,
                   singularity_available=singularity_available,
                   prefer_singularity=prefer_singularity)

        # Primary selection logic - FORCE SINGULARITY due to Docker user namespace issues
        if singularity_available:
            logger.info("using_singularity_as_primary_runtime_forced_due_to_docker_issues")
            return "singularity"

        if docker_available:
            # Docker as fallback (may have user namespace issues)
            logger.warning("using_docker_as_fallback_runtime_singularity_preferred_but_unavailable")
            return "docker"

        if singularity_available:
            # Docker not available, try Singularity as fallback
            if self._find_singularity_image():
                logger.info("using_singularity_as_docker_fallback")
                return "singularity"
            else:
                logger.warning("singularity_available_but_no_image_found")

        # No container runtimes available
        logger.warning("no_container_runtimes_available_for_freesurfer")

        # No working FreeSurfer runtime found
        logger.warning("no_freesurfer_runtime_available_will_use_mock_data")
        return "none"

    # ===== CONCURRENCY CONTROL METHODS =====

    def _check_container_concurrency_limit(self) -> None:
        """
        Check if launching another FreeSurfer container would exceed concurrency limits.

        Raises RuntimeError if the limit would be exceeded, preventing resource exhaustion.
        This enforces the max_concurrent_jobs setting at the container orchestration level.
        """
        try:
            # Get current running FreeSurfer containers
            result = subprocess_module.run(
                ["docker", "ps", "--filter", "name=freesurfer-job-", "--format", "{{.Names}}"],
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode == 0:
                running_containers = [line.strip() for line in result.stdout.strip().split('\n') if line.strip()]
                current_count = len(running_containers)

                logger.info("container_concurrency_check",
                          current_running=current_count,
                          max_allowed=settings.max_concurrent_jobs,
                          job_id=str(self.job_id))

                if current_count >= settings.max_concurrent_jobs:
                    running_names = ", ".join(running_containers) if running_containers else "none"
                    raise RuntimeError(
                        f"Container concurrency limit exceeded. "
                        f"Currently running: {current_count} FreeSurfer containers ({running_names}). "
                        f"Maximum allowed: {settings.max_concurrent_jobs}. "
                        f"Please wait for existing jobs to complete before starting new ones."
                    )

                logger.info("container_concurrency_check_passed",
                          current_running=current_count,
                          max_allowed=settings.max_concurrent_jobs,
                          job_id=str(self.job_id))
            else:
                # If docker ps fails, log warning but allow processing to continue
                logger.warning("container_concurrency_check_failed",
                             error=result.stderr.strip(),
                             message="Could not check running containers, proceeding with caution")

        except RuntimeError:
            # Re-raise RuntimeError (concurrency limit exceeded) to block processing
            raise
        except subprocess_module.TimeoutExpired:
            logger.warning("container_concurrency_check_timeout",
                         message="Docker ps command timed out, proceeding with caution")
        except Exception as e:
            logger.warning("container_concurrency_check_error",
                         error=str(e),
                         message="Error checking container concurrency, proceeding with caution")

    # ===== FREESURFER FALLBACK METHODS =====

    def _is_freesurfer_available(self) -> bool:
        """Check if FreeSurfer can be used as fallback."""
        try:
            # Must have Docker (for now - could add Singularity later)
            if not self._is_docker_available():
                logger.warning("freesurfer_unavailable_no_docker",
                              message="FreeSurfer requires Docker, but Docker is not available or not running. "
                                     "Please install Docker Desktop or ensure Docker daemon is running.")
                return False

            # Must have license
            license_path = self._get_freesurfer_license_path()
            if not license_path:
                logger.warning("freesurfer_unavailable_no_license",
                              message="FreeSurfer license not found. Processing will continue with alternative methods. "
                                     "To enable FreeSurfer, place license.txt in the app folder or set FREESURFER_LICENSE environment variable.")
                return False

            logger.debug("freesurfer_available_for_fallback")
            return True

        except Exception as e:
            logger.warning("freesurfer_availability_check_failed",
                          error=str(e),
                          message="Failed to check FreeSurfer availability due to unexpected error.")
            return False

    def _check_docker_available(self) -> bool:
        """Check if Docker is available and functioning."""
        try:
            import subprocess
            result = subprocess.run(
                ["docker", "info"],
                capture_output=True,
                timeout=10
            )
            if result.returncode == 0:
                logger.debug("docker_available")
                return True
            else:
                logger.warning("docker_unavailable",
                             stderr=result.stderr.decode()[:200] if result.stderr else "Unknown error")
                return False
        except FileNotFoundError:
            logger.warning("docker_not_installed",
                          message="Docker is not installed. Install Docker to enable FreeSurfer processing.")
            return False
        except Exception as e:
            logger.warning("docker_check_failed", error=str(e))
            return False

    def _get_freesurfer_license_path(self) -> Path:
        """Get FreeSurfer license path from multiple possible locations.

        Searches in the same locations as the license API for consistency.
        """
        # Use the same search logic as the license API
        base_dir = Path(__file__).parent.parent.parent  # desktop_alone_web directory
        search_paths = [
            base_dir / "license.txt",  # Primary location for users
            base_dir / "freesurfer_license.txt",  # Legacy support
            base_dir / "resources" / "licenses" / "license.txt",
            base_dir / "resources" / "licenses" / "freesurfer_license.txt",
            Path.home() / "neuroinsight" / "resources" / "licenses" / "license.txt",
            Path.home() / "neuroinsight" / "license.txt",
            Path("/usr/local/freesurfer/license.txt"),  # System FreeSurfer location
        ]

        # Also check environment variable
        import os
        license_env = os.getenv('FREESURFER_LICENSE')
        if license_env and Path(license_env).exists():
            logger.debug("freesurfer_license_found_via_env", path=license_env)
            return Path(license_env)

        # Check all search paths
        for license_path in search_paths:
            if license_path.exists():
                logger.debug("freesurfer_license_found", path=str(license_path))
                return license_path

        logger.debug("freesurfer_license_not_found")
        return None
        if legacy_app_license.exists():
            logger.debug("freesurfer_license_found_legacy_app", path=str(legacy_app_license))
            return legacy_app_license

        logger.debug("freesurfer_license_not_found")
        logger.warning("freesurfer_no_license_detected",
                      message="No FreeSurfer license found in any location. "
                             "Users should place license.txt in the app folder or set FREESURFER_LICENSE environment variable.")
        return None

    def _get_app_root_directory(self) -> Path:
        """Get the application root directory (works for both development and deployed apps)."""
        # Get the directory containing the MRI processor module
        processor_dir = Path(__file__).parent  # pipeline/processors/
        pipeline_dir = processor_dir.parent     # pipeline/
        app_root = pipeline_dir.parent          # desktop_alone_web/

        # For deployed apps, check if we're in a bundled environment
        if app_root.name == "desktop_alone_web":
            return app_root
        else:
            # Try to find the app root by going up directories
            current = Path.cwd()
            for _ in range(5):  # Don't go up more than 5 levels
                if (current / "resources" / "licenses").exists():
                    return current
                current = current.parent

        # Fallback to current working directory approach
        return Path.cwd()

    def _ensure_container_image(self, image_name: str, display_name: str, size_gb: int = 4) -> None:
        """Generic lazy container download with progress reporting."""
        try:
            # Check if image exists
            result = subprocess_module.run(
                ["docker", "images", "-q", image_name],
                capture_output=True, timeout=10
            )

            if not result.stdout.strip():
                logger.info(f"{image_name.replace('/', '_')}_image_missing_starting_download")

                # Show progress to user
                if self.progress_callback:
                    self.progress_callback(
                        self._get_current_progress(),
                        f"üê≥ Downloading {display_name} ({size_gb}GB, one-time - 10-15 min)..."
                    )

                # Download with timeout - disable TTY requirements
                # Enhanced Docker image download with progress messages
                self._download_docker_image_with_progress(image_name, display_name)

                if result.returncode == 0:
                    logger.info(f"{image_name.replace('/', '_')}_download_successful")
                    if self.progress_callback:
                        self.progress_callback(
                            self._get_current_progress(),
                            f" {display_name} ready - continuing processing..."
                        )
                else:
                    error_msg = result.stderr.decode() if result.stderr else "Unknown error"
                    logger.error(f"{image_name.replace('/', '_')}_download_failed", error=error_msg)

                    # Check for common Docker environment issues
                    if "short-name resolution enforced" in error_msg:
                        error_msg += " (Docker requires TTY for interactive prompts. Try running from a terminal.)"
                    elif "insufficient UIDs or GIDs" in error_msg:
                        error_msg += " (Container UID/GID mapping issue in this environment.)"

                    raise RuntimeError(f"{display_name} download failed: {error_msg}")

        except subprocess_module.TimeoutExpired:
            logger.error(f"{image_name.replace('/', '_')}_download_timeout")
            raise RuntimeError(f"{display_name} download timed out after {FREESURFER_DOWNLOAD_TIMEOUT_MINUTES} minutes")

    def _run_freesurfer_fallback(self, nifti_path: Path, output_dir: Path) -> Path:
        """Execute FreeSurfer segmentation with automatic runtime selection and fallbacks."""
        logger.info("starting_freesurfer_fallback", input=str(nifti_path))

        # Get license path early
        license_path = self._get_freesurfer_license_path()
        if not license_path:
            raise RuntimeError("FreeSurfer license not found")

        # Use the new intelligent runtime selection
        freesurfer_runtime = self._check_container_runtime_availability()
        attempted_runtimes = []

        logger.info("freesurfer_runtime_selected", runtime=freesurfer_runtime)

        # COMMENTED OUT: Nipype requires manual FreeSurfer installation
        # Keeping containers as primary method for easier deployment
        # try:
        #     logger.info("attempting_freesurfer_nipype_first")
        #     attempted_runtimes.append("nipype")
        #     return self._run_freesurfer_nipype(nifti_path, output_dir, license_path)
        # except Exception as nipype_error:
        #     logger.warning("freesurfer_nipype_failed", error=str(nipype_error), error_type=type(nipype_error).__name__)

        # Try selected runtime with intelligent fallback logic
        if freesurfer_runtime == "docker":
            logger.info("attempting_freesurfer_docker_primary")
            attempted_runtimes.append("docker")
            try:
                return self._run_freesurfer_docker(nifti_path, output_dir, license_path)
            except Exception as docker_error:
                logger.warning("freesurfer_docker_failed", error=str(docker_error), error_type=type(docker_error).__name__)

                # Try Singularity fallback
                if self._is_singularity_available() and self._find_singularity_image():
                    logger.info("docker_failed_trying_singularity_fallback")
                    attempted_runtimes.append("singularity")
                    try:
                        return self._run_freesurfer_singularity(nifti_path, output_dir)
                    except Exception as sing_error:
                        logger.warning("singularity_fallback_failed", error=str(sing_error), error_type=type(sing_error).__name__)

                # Native FreeSurfer support removed - only container methods available

        elif freesurfer_runtime == "singularity":
            logger.info("attempting_freesurfer_singularity_primary")
            attempted_runtimes.append("singularity")
            try:
                return self._run_freesurfer_singularity(nifti_path, output_dir)
            except Exception as sing_error:
                logger.warning("freesurfer_singularity_failed", error=str(sing_error), error_type=type(sing_error).__name__)

                # Try Docker fallback
                if self._is_docker_available():
                    logger.info("singularity_failed_trying_docker_fallback")
                    attempted_runtimes.append("docker")
                    try:
                        return self._run_freesurfer_docker(nifti_path, output_dir, license_path)
                    except Exception as docker_error:
                        logger.warning("docker_fallback_failed", error=str(docker_error), error_type=type(docker_error).__name__)

                # Native FreeSurfer support removed - only container methods available

        # Native FreeSurfer support removed - only container methods supported

        # Secondary attempt with Singularity only
        elif freesurfer_runtime == "singularity":
            logger.info("attempting_freesurfer_singularity_as_primary")
            attempted_runtimes.append("singularity")
            try:
                return self._run_freesurfer_singularity(nifti_path, output_dir)
            except Exception as sing_error:
                logger.warning("freesurfer_singularity_failed", error=str(sing_error))

        # If we get here, both runtimes failed
        logger.warning("all_freesurfer_runtimes_failed",
                      attempted_runtimes=attempted_runtimes,
                      note="FreeSurfer fallback failed, will use mock data")

        # This will be handled by the caller - we raise an exception to indicate FreeSurfer failed
        raise RuntimeError(f"FreeSurfer processing failed with all available runtimes: {attempted_runtimes}")

    # _run_freesurfer_native method removed - native FreeSurfer support disabled
    def _find_freesurfer_sif(self) -> Path:
        """Find local FreeSurfer .sif container file."""
        # Get the application directory
        app_dir = Path(__file__).parent.parent.parent  # Go up from processors/mri_processor.py

        # Check multiple possible locations for the .sif file
        search_paths = [
            # HPC FreeSurfer containers (discovered on this system)
            Path("/opt/ood/images/freesurfer/freesurfer_7.4.1.sif"),
            Path("/opt/ood_apps/images/freesurfer/freesurfer_7.4.1.sif"),
            # Common HPC container locations
            Path("/shared/containers/freesurfer/freesurfer.sif"),
            Path("/opt/containers/freesurfer/freesurfer.sif"),
            Path("/usr/local/containers/freesurfer/freesurfer.sif"),
            # Same directory as the application
            Path("./freesurfer.sif"),
            Path("./freesurfer-7.3.2.sif"),
            Path("./freesurfer-7.4.1.sif"),
            # In conda environment
            Path(os.getenv('CONDA_PREFIX', '')) / "share" / "freesurfer.sif" if os.getenv('CONDA_PREFIX') else None,
            # In package directory
            app_dir / "freesurfer.sif",
            app_dir / "freesurfer-7.3.2.sif",
            app_dir / "freesurfer-7.4.1.sif",
            # In distribution package
            app_dir.parent / "freesurfer.sif",
            app_dir.parent / "freesurfer-7.4.1.sif",
        ]

        for path in search_paths:
            if path and path.exists():
                logger.info("freesurfer_sif_found", path=str(path))
                return path

        # If no SIF file found, try to download and convert Docker image
        logger.info("no_local_sif_found_attempting_download")
        downloaded_sif = self._download_freesurfer_apptainer()
        if downloaded_sif and downloaded_sif.exists():
            logger.info("freesurfer_sif_downloaded", path=str(downloaded_sif))
            return downloaded_sif

        logger.warning("no_freesurfer_sif_available")
        return None

    def _ensure_singularity_container(self) -> Path:
        """Ensure FreeSurfer Singularity container is available, downloading if necessary."""
        app_dir = Path(__file__).parent.parent.parent
        target_sif = app_dir / "freesurfer-7.4.1.sif"
        download_script = app_dir / "download_freesurfer_apptainer.sh"

        # Check if container already exists
        if target_sif.exists():
            logger.info("freesurfer_singularity_container_already_exists", path=str(target_sif))
            return target_sif

        # Check if download script exists
        if not download_script.exists():
            logger.warning("singularity_download_script_not_found",
                         script_path=str(download_script),
                         message="Cannot automatically download FreeSurfer Singularity container")
            return None

        logger.info("starting_freesurfer_singularity_download",
                   target=str(target_sif),
                   script=str(download_script))

        # Show progress to user
        if self.progress_callback:
            self.progress_callback(
                self._get_current_progress(),
                f"‚¨áÔ∏è Downloading FreeSurfer Singularity container (~4GB, one-time - 10-15 min)..."
            )

        try:
            import subprocess

            # Run the download script
            result = subprocess.run(
                ["bash", str(download_script)],
                cwd=app_dir,
                capture_output=True,
                text=True,
                timeout=1800  # 30 minutes timeout for download
            )

            if result.returncode == 0 and target_sif.exists():
                logger.info("freesurfer_singularity_download_successful", sif_path=str(target_sif))

                if self.progress_callback:
                    self.progress_callback(
                        self._get_current_progress(),
                        f"‚úÖ FreeSurfer Singularity container ready - continuing processing..."
                    )

                return target_sif
            else:
                logger.error("freesurfer_singularity_download_failed",
                           returncode=result.returncode,
                           stdout=result.stdout.strip(),
                           stderr=result.stderr.strip())
                return None

        except subprocess.TimeoutExpired:
            logger.error("freesurfer_singularity_download_timeout", timeout_minutes=30)
            return None
        except Exception as e:
            logger.error("freesurfer_singularity_download_error", error=str(e))
            return None

    def _download_freesurfer_apptainer(self) -> Path:
        """Download FreeSurfer Docker image and convert to Apptainer format."""
        # This method is now deprecated - use _ensure_singularity_container() instead
        return self._ensure_singularity_container()

    def _run_freesurfer_nipype(self, nifti_path: Path, output_dir: Path, license_path: Path) -> Path:
        """Execute FreeSurfer segmentation using nipype within conda environment."""
        subject_id = f"freesurfer_nipype_{self.job_id}"
        freesurfer_dir = output_dir / "freesurfer_nipype"
        freesurfer_dir.mkdir(exist_ok=True)

        # Update progress
        if self.progress_callback:
            self.progress_callback(
                self._get_current_progress(),
                f"Processing with FreeSurfer (Nipype) ({subject_id})..."
            )

        try:
            # Check for local .sif file first
            sif_path = self._find_freesurfer_sif()
            if sif_path:
                logger.info("using_local_freesurfer_container", sif_path=str(sif_path))
                return self._run_freesurfer_singularity_local(nifti_path, freesurfer_dir, license_path, sif_path)

            # COMMENTED OUT: No longer falling back to native Nipype
            # This ensures we only use containers for FreeSurfer processing
            # logger.info("no_local_container_found_trying_nipype")
            # return self._run_freesurfer_nipype_system(nifti_path, freesurfer_dir, license_path)

            # If no local containers found, let the main logic handle container fallbacks
            raise Exception("No local FreeSurfer containers found - will fallback to main container logic")

        except Exception as e:
            logger.error("freesurfer_nipype_failed",
                        subject_id=subject_id,
                        error=str(e),
                        error_type=type(e).__name__)
            # Fall back to mock data on failure
            logger.warning("freesurfer_nipype_failed_using_mock_data")
            self._create_mock_freesurfer_output(freesurfer_dir)
            return freesurfer_dir

    def _run_freesurfer_nipype_system(self, nifti_path: Path, freesurfer_dir: Path, license_path: Path) -> Path:
        """Run FreeSurfer using nipype with system installation."""
        subject_id = f"freesurfer_nipype_{self.job_id}"

        try:
            # Import nipype FreeSurfer interface
            from nipype.interfaces.freesurfer import ReconAll

            # Set FreeSurfer environment
            import os

            # Check for local FreeSurfer installation in project directory
            local_freesurfer = self.app_dir / "freesurfer"  # Directory name after extraction
            if local_freesurfer.exists() and (local_freesurfer / "bin" / "recon-all").exists():
                logger.info("using_local_freesurfer_installation", path=str(local_freesurfer))
                os.environ['FREESURFER_HOME'] = str(local_freesurfer)
                os.environ['PATH'] = f"{local_freesurfer}/bin:{os.environ.get('PATH', '')}"
            else:
                logger.info("using_system_freesurfer_installation")

            # Ensure license is accessible - copy to FREESURFER_HOME and use absolute path
            fs_license_dest = local_freesurfer / 'license.txt'
            if not fs_license_dest.exists():
                import shutil
                shutil.copy2(license_path, fs_license_dest)
            os.environ['FS_LICENSE'] = str(fs_license_dest)

            # Create subjects directory if it doesn't exist
            freesurfer_dir.mkdir(parents=True, exist_ok=True)
            os.environ['SUBJECTS_DIR'] = str(freesurfer_dir)

            # Create ReconAll interface
            recon = ReconAll()
            recon.inputs.subject_id = subject_id
            recon.inputs.subjects_dir = str(freesurfer_dir)
            recon.inputs.T1_files = str(nifti_path)

            # Configure for complete segmentation (autorecon1 + autorecon2-volonly)
            recon.inputs.directive = 'autorecon1'  # Initial processing (skull stripping, basic segmentation)
            recon.inputs.flags = ['-autorecon2-volonly']  # Volume refinement (complete segmentation, no surfaces)

            # Set environment explicitly for nipype to ensure license is found
            import shutil
            env = os.environ.copy()
            fs_license_path = None

            if local_freesurfer.exists():
                env['FREESURFER_HOME'] = str(local_freesurfer)
                env['PATH'] = f"{local_freesurfer}/bin:{env.get('PATH', '')}"

                # Copy license to FREESURFER_HOME directory and use absolute path
                fs_license_path = local_freesurfer / 'license.txt'
                if not fs_license_path.exists():
                    shutil.copy2(license_path, fs_license_path)
                    logger.info("copied_license_to_freesurfer_home", path=str(fs_license_path))

                # Also copy to subjects directory (where FreeSurfer runs)
                subj_license_path = freesurfer_dir / 'license.txt'
                if not subj_license_path.exists():
                    shutil.copy2(license_path, subj_license_path)

            # Use absolute path for FS_LICENSE
            env['FS_LICENSE'] = str(fs_license_path) if fs_license_path else str(license_path)
            env['SUBJECTS_DIR'] = str(freesurfer_dir)
            recon.inputs.environ = env

            logger.info("executing_freesurfer_nipype_system",
                       subject_id=subject_id,
                       input_file=str(nifti_path),
                       output_dir=str(freesurfer_dir))

            # Execute with nipype
            logger.info("starting_nipype_recon_run", subject_id=subject_id)
            result = recon.run()
            logger.info("nipype_recon_run_returned", subject_id=subject_id, result_type=type(result).__name__)

            # Log detailed result information
            if hasattr(result, 'outputs'):
                logger.info("nipype_result_outputs", subject_id=subject_id, outputs=str(result.outputs))
            if hasattr(result, 'runtime'):
                logger.info("nipype_result_runtime", subject_id=subject_id, runtime=str(result.runtime))

            logger.info("freesurfer_nipype_system_completed",
                       subject_id=subject_id,
                       result=str(result))

            # Verify output exists and try to extract hippocampus data
            subject_output_dir = freesurfer_dir / subject_id
            logger.info("checking_subject_output_dir",
                       subject_id=subject_id,
                       output_dir=str(subject_output_dir),
                       exists=subject_output_dir.exists())

            if subject_output_dir.exists():
                # Try to extract hippocampus data from FreeSurfer output
                hippo_data = self._extract_freesurfer_hippocampus_data(freesurfer_dir, subject_id)
                if hippo_data:
                    logger.info("freesurfer_partial_success",
                               subject_id=subject_id,
                               extracted_volumes=hippo_data,
                               message="FreeSurfer completed with usable segmentation data")
                    return freesurfer_dir
                else:
                    logger.warning("freesurfer_no_hippocampus_data",
                                 subject_id=subject_id,
                                 message="FreeSurfer completed but no hippocampus data found")
                    # Continue to mock fallback
            else:
                logger.warning("freesurfer_no_output_directory",
                             subject_id=subject_id,
                             expected_dir=str(subject_output_dir))

            # If we get here, FreeSurfer didn't produce usable results
            logger.info("freesurfer_incomplete_falling_back_to_mock",
                       subject_id=subject_id,
                       message="FreeSurfer processing incomplete, using mock data")
            self._create_mock_freesurfer_output(freesurfer_dir)
            return freesurfer_dir

        except Exception as e:
            logger.warning("freesurfer_nipype_system_failed",
                         subject_id=subject_id,
                         error=str(e),
                         error_type=type(e).__name__,
                         message="FreeSurfer nipype execution failed, checking for partial results")

            # Try to extract hippocampus data from any partial FreeSurfer output
            hippo_data = self._extract_freesurfer_hippocampus_data(freesurfer_dir, subject_id)
            if hippo_data:
                logger.info("freesurfer_partial_success_with_error",
                           subject_id=subject_id,
                           extracted_volumes=hippo_data,
                           error=str(e),
                           message="FreeSurfer failed but produced usable hippocampus data")
                return freesurfer_dir
            else:
                logger.warning("freesurfer_failed_no_usable_data",
                             subject_id=subject_id,
                             error=str(e),
                             message="FreeSurfer failed and no usable data found, using mock data")
                self._create_mock_freesurfer_output(freesurfer_dir)
                return freesurfer_dir

        except ImportError:
            logger.warning("nipype_not_available", message="Falling back to mock data")
            # Fall back to mock data if nipype isn't available
            self._create_mock_freesurfer_output(freesurfer_dir)
            return freesurfer_dir

    def _run_freesurfer_singularity_local(self, nifti_path: Path, freesurfer_dir: Path, license_path: Path, sif_path: Path) -> Path:
        """Run FreeSurfer using local Singularity container."""
        subject_id = f"freesurfer_singularity_{self.job_id}"

        # Ensure the FreeSurfer directory exists (apptainer requires it for mounting)
        freesurfer_dir.mkdir(parents=True, exist_ok=True)

        # Update progress
        if self.progress_callback:
            self.progress_callback(
                self._get_current_progress(),
                f"Processing with FreeSurfer (Singularity) ({subject_id})..."
            )

        import subprocess
        import os

        # IMPORTANT: Clean up any existing subject directory to prevent "re-run existing subject" error
        subject_output_dir = freesurfer_dir / subject_id
        if subject_output_dir.exists():
            logger.warning("freesurfer_subject_dir_exists",
                          path=str(subject_output_dir),
                          message="Removing existing subject directory to allow recon-all -i to run")
            import shutil
            shutil.rmtree(subject_output_dir)

        # Set up Singularity command combining autorecon1 and autorecon2-volonly
        # Convert all paths to absolute paths (required for container mounts)
        abs_freesurfer_dir = freesurfer_dir.resolve()
        abs_input_dir = nifti_path.parent.resolve()
        abs_license_path = license_path.resolve()
        abs_sif_path = sif_path.resolve()
        
        singularity_cmd = [
            "apptainer", "exec",  # or "singularity"
            # Removed --cleanenv to preserve FreeSurfer environment
            "--bind", f"{abs_freesurfer_dir}:/subjects",
            "--bind", f"{abs_input_dir}:/input:ro",
            "--bind", f"{abs_license_path}:/usr/local/freesurfer/license.txt:ro",
            "--env", f"FS_LICENSE=/usr/local/freesurfer/license.txt",
            "--env", f"SUBJECTS_DIR=/subjects",
            str(abs_sif_path),  # Local .sif file
            "recon-all",
            "-i", f"/input/{nifti_path.name}",
            "-s", subject_id,
            "-autorecon1",
            "-autorecon2-volonly"  # Combined in single command as requested
        ]

        logger.info("executing_freesurfer_singularity_combined",
                   command=" ".join(singularity_cmd),
                   subject_id=subject_id,
                   sif_path=str(sif_path),
                   input_file=str(nifti_path))

        try:
            # Execute combined autorecon1 + autorecon2-volonly
            logger.info("freesurfer_singularity_starting_combined_autorecon",
                       command=" ".join(singularity_cmd),
                       nifti_path=str(nifti_path),
                       sif_path=str(sif_path))

            result = subprocess.run(
                singularity_cmd,
                capture_output=True,
                timeout=FREESURFER_PROCESSING_TIMEOUT_MINUTES*60,  # Use the configured timeout
                text=True,
                env=self._get_extended_env()
            )

            if result.returncode != 0:
                logger.error("freesurfer_combined_autorecon_failed",
                           returncode=result.returncode,
                           stderr=result.stderr[:1000],
                           stdout=result.stdout[:1000])
                raise RuntimeError(f"FreeSurfer combined autorecon failed: {result.stderr[:200]}")

            logger.info("freesurfer_combined_autorecon_completed")

            # Now run mri_segstats to generate aseg.stats from aseg.auto.mgz
            subject_mri_dir = freesurfer_dir / subject_id / "mri"
            subject_stats_dir = freesurfer_dir / subject_id / "stats"
            subject_stats_dir.mkdir(exist_ok=True, parents=True)

            aseg_auto_mgz = subject_mri_dir / "aseg.auto.mgz"
            aseg_stats = subject_stats_dir / "aseg.stats"
            brain_mgz = subject_mri_dir / "brain.mgz"

            if aseg_auto_mgz.exists():
                # Run mri_segstats to generate aseg.stats
                # Use absolute paths for container mounts
                abs_freesurfer_dir = freesurfer_dir.resolve()
                abs_license_path = license_path.resolve()
                abs_sif_path = sif_path.resolve()
                
                segstats_cmd = [
                    "apptainer", "exec",
                    "--bind", f"{abs_freesurfer_dir}:/subjects",
                    "--bind", f"{abs_license_path}:/usr/local/freesurfer/license.txt:ro",
                    "--env", f"FS_LICENSE=/usr/local/freesurfer/license.txt",
                    "--env", f"SUBJECTS_DIR=/subjects",
                    str(abs_sif_path),
                    "mri_segstats",
                    "--seg", f"/subjects/{subject_id}/mri/aseg.auto.mgz",
                    "--excludeid", "0",
                    "--sum", f"/subjects/{subject_id}/stats/aseg.stats",
                    "--i", f"/subjects/{subject_id}/mri/brain.mgz"
                ]

                logger.info("running_mri_segstats_after_combined_autorecon", command=" ".join(segstats_cmd))
                segstats_result = subprocess.run(
                    segstats_cmd,
                    capture_output=True,
                    timeout=300,  # 5 minutes for stats generation
                    text=True,
                    env=self._get_extended_env()
                )

                if segstats_result.returncode != 0:
                    logger.warning("mri_segstats_failed_after_combined_autorecon",
                                 returncode=segstats_result.returncode,
                                 stderr=segstats_result.stderr[:500])
                    # Don't raise error - continue with processing, fallback will handle missing stats
                else:
                    logger.info("mri_segstats_completed_after_combined_autorecon")
            else:
                logger.warning("aseg.auto.mgz_not_found_after_combined_autorecon", path=str(aseg_auto_mgz))

            # Use combined autorecon result for final check
            logger.info("freesurfer_singularity_command_result",
                       returncode=result.returncode,
                       stdout_length=len(result.stdout),
                       stderr_length=len(result.stderr))
            if result.returncode != 0:
                logger.error("freesurfer_combined_autorecon_failed",
                           returncode=result.returncode,
                           stderr=result.stderr[:1000],
                           stdout=result.stdout[:1000],
                           command=" ".join(singularity_cmd))

            if result.returncode == 0:
                logger.info("freesurfer_singularity_completed",
                           subject_id=subject_id,
                           stdout_lines=len(result.stdout.split('\n')))

                # Verify output exists
                subject_output_dir = freesurfer_dir / subject_id
                if subject_output_dir.exists():
                    return freesurfer_dir
                else:
                    raise RuntimeError(f"FreeSurfer completed but output directory {subject_output_dir} not found")
            else:
                error_msg = result.stderr or result.stdout
                logger.error("freesurfer_singularity_failed",
                           returncode=result.returncode,
                           error=error_msg[:500])
                raise RuntimeError(f"FreeSurfer Singularity failed: {error_msg[:200]}")

        except subprocess.TimeoutExpired:
            logger.error("freesurfer_singularity_timeout",
                        subject_id=subject_id,
                        timeout_minutes=FREESURFER_PROCESSING_TIMEOUT_MINUTES)
            raise RuntimeError(f"FreeSurfer processing timed out after {FREESURFER_PROCESSING_TIMEOUT_MINUTES} minutes")
        except FileNotFoundError as fnf_error:
            logger.error("apptainer_command_not_found",
                        error=str(fnf_error),
                        command=singularity_cmd[0])
            logger.warning("apptainer_not_found", message="Trying singularity command")
            # Try with singularity instead of apptainer
            singularity_cmd[0] = "singularity"
            try:
                result = subprocess.run(
                    singularity_cmd,
                    capture_output=True,
                    timeout=FREESURFER_PROCESSING_TIMEOUT_MINUTES*60,
                    text=True,
                    env=self._get_extended_env()
                )
                if result.returncode == 0:
                    subject_output_dir = freesurfer_dir / subject_id
                    if subject_output_dir.exists():
                        return freesurfer_dir
            except:
                pass

            raise RuntimeError("Neither apptainer nor singularity found")

    def _run_freesurfer_docker(self, nifti_path: Path, output_dir: Path, license_path: Path) -> Path:
        """Execute complete FreeSurfer segmentation using Docker (autorecon1 + autorecon2-volonly + mri_segstats)."""

        # Check Docker availability before proceeding
        if not self._check_docker_available():
            raise RuntimeError(
                "Docker is not available. FreeSurfer processing requires Docker. "
                "Please ensure Docker is installed and running: 'sudo systemctl start docker'"
            )

        subject_id = f"freesurfer_docker_{self.job_id}"
        freesurfer_dir = output_dir / "freesurfer_docker"
        freesurfer_dir.mkdir(exist_ok=True)

        # Ensure FreeSurfer container is available (lazy download)
        self._ensure_container_image(
            FREESURFER_CONTAINER_IMAGE,
            "FreeSurfer",
            FREESURFER_CONTAINER_SIZE_GB
        )

        # Update progress
        if self.progress_callback:
            self.progress_callback(
                self._get_current_progress(),
                f"Processing with FreeSurfer (Docker) ({subject_id})..."
            )

        # IMPORTANT: Clean up any existing subject directory to prevent "re-run existing subject" error
        subject_output_dir = freesurfer_dir / subject_id
        if subject_output_dir.exists():
            logger.warning("freesurfer_subject_dir_exists",
                          path=str(subject_output_dir),
                          message="Removing existing subject directory to allow recon-all -i to run")
            import shutil
            shutil.rmtree(subject_output_dir)

        # Execute with timeout
        try:
            # FreeSurfer Docker command combining autorecon1 and autorecon2-volonly
            # Convert all paths to absolute paths (Docker requires absolute paths for volume mounts)
            abs_freesurfer_dir = freesurfer_dir.resolve()
            abs_input_dir = nifti_path.parent.resolve()
            abs_license_path = license_path.resolve()
            
            # Use a unique container name so we can track and kill it if needed
            container_name = f"freesurfer-job-{self.job_id}"

            # ENFORCE CONTAINER CONCURRENCY LIMIT
            self._check_container_concurrency_limit()

            docker_cmd = [
                "docker", "run", "--rm", "--user", "root",
                "--name", container_name,  # Named container for tracking
                "-v", f"{abs_freesurfer_dir}:/subjects",
                "-v", f"{abs_input_dir}:/input:ro",
                "-v", f"{abs_license_path}:/usr/local/freesurfer/license.txt:ro",
                "-e", "FS_LICENSE=/usr/local/freesurfer/license.txt",
                "-e", "SUBJECTS_DIR=/subjects",
                FREESURFER_CONTAINER_IMAGE,
                "recon-all",
                "-i", f"/input/{nifti_path.name}",
                "-s", subject_id,
                "-autorecon1",
                "-autorecon2-volonly"  # Combined in single command as requested
            ]
            
            # Store container name in database for cancellation support
            self._store_container_id(container_name)

            logger.info("executing_freesurfer_docker_combined_autorecon",
                       command=" ".join(docker_cmd),
                       subject_id=subject_id,
                       timeout_minutes=FREESURFER_PROCESSING_TIMEOUT_MINUTES)

            # Start progress monitoring
            status_log_path = subject_output_dir / "scripts" / "recon-all-status.log"
            # Convert to absolute path so monitor thread can find it reliably
            abs_status_log_path = status_log_path.resolve()
            progress_monitor = self._start_freesurfer_progress_monitor(abs_status_log_path, base_progress=self._get_current_progress())

            # Run combined autorecon1 + autorecon2-volonly
            logger.info("freesurfer_docker_starting_combined_autorecon",
                       command=" ".join(docker_cmd))

            result = subprocess_module.run(
                docker_cmd,
                capture_output=True,
                timeout=FREESURFER_PROCESSING_TIMEOUT_MINUTES*60,
                env=self._get_extended_env()
            )

            if result.returncode != 0:
                stderr_output = result.stderr.decode() if result.stderr else ""
                stdout_output = result.stdout.decode() if result.stdout else ""

                # Extract more detailed error information
                error_details = []
                if "license" in stderr_output.lower():
                    error_details.append("FreeSurfer license issue - check license.txt file")
                if "no such file" in stderr_output.lower():
                    error_details.append("Input file not found in container")
                if "permission denied" in stderr_output.lower():
                    error_details.append("Docker permission issue - check user permissions")
                if "no space left" in stderr_output.lower():
                    error_details.append("Insufficient disk space for processing")

                detailed_error = "; ".join(error_details) if error_details else "Check Docker logs for details"

                error_msg = f"FreeSurfer Docker failed (exit code: {result.returncode}). {detailed_error}"
                if stderr_output:
                    error_msg += f" STDERR: {stderr_output[:300]}..."

                logger.error("freesurfer_docker_combined_autorecon_failed",
                           returncode=result.returncode,
                           stderr=stderr_output[:500],
                           stdout=stdout_output[:500],
                           error_details=error_details)

                raise RuntimeError(error_msg)

            logger.info("freesurfer_docker_combined_autorecon_completed")

            # Now run mri_segstats to generate aseg.stats from aseg.auto.mgz
            subject_mri_dir = freesurfer_dir / subject_id / "mri"
            subject_stats_dir = freesurfer_dir / subject_id / "stats"
            subject_stats_dir.mkdir(exist_ok=True, parents=True)

            aseg_auto_mgz = subject_mri_dir / "aseg.auto.mgz"
            aseg_stats = subject_stats_dir / "aseg.stats"
            brain_mgz = subject_mri_dir / "brain.mgz"

            if aseg_auto_mgz.exists():
                # Run mri_segstats to generate aseg.stats
                # Use absolute paths for Docker volume mounts
                abs_freesurfer_dir = freesurfer_dir.resolve()
                abs_license_path = license_path.resolve()
                
                segstats_cmd = [
                    "docker", "run", "--rm", "--user", "root",
                    "-v", f"{abs_freesurfer_dir}:/subjects",
                    "-v", f"{abs_license_path}:/usr/local/freesurfer/license.txt:ro",
                    "-e", "FS_LICENSE=/usr/local/freesurfer/license.txt",
                    "-e", "SUBJECTS_DIR=/subjects",
                    FREESURFER_CONTAINER_IMAGE,
                    "mri_segstats",
                    "--seg", f"/subjects/{subject_id}/mri/aseg.auto.mgz",
                    "--excludeid", "0",
                    "--sum", f"/subjects/{subject_id}/stats/aseg.stats",
                    "--i", f"/subjects/{subject_id}/mri/brain.mgz"
                ]

                logger.info("running_docker_mri_segstats_after_combined_autorecon", command=" ".join(segstats_cmd))
                segstats_result = subprocess_module.run(
                    segstats_cmd,
                    capture_output=True,
                    timeout=300,  # 5 minutes for stats generation
                    env=self._get_extended_env()
                )

                if segstats_result.returncode != 0:
                    error_msg = segstats_result.stderr.decode()[:500] if segstats_result.stderr else "Unknown error"
                    logger.warning("docker_mri_segstats_failed_after_combined_autorecon",
                                 returncode=segstats_result.returncode,
                                 error=error_msg)
                    # Don't raise error - continue with processing, fallback will handle missing stats
                else:
                    logger.info("docker_mri_segstats_completed_after_combined_autorecon")
            else:
                logger.warning("aseg.auto.mgz_not_found_after_combined_autorecon_in_docker", path=str(aseg_auto_mgz))

            if result.returncode == 0:
                logger.info("freesurfer_docker_processing_completed", subject_id=subject_id)
                return freesurfer_dir

            else:
                error_msg = result.stderr.decode()[:500] if result.stderr else "Unknown error"
                logger.error("freesurfer_docker_autorecon2_failed",
                           returncode=result.returncode,
                           error=error_msg,
                           subject_id=subject_id)
                raise RuntimeError(f"FreeSurfer Docker autorecon2 failed: {error_msg}")

        except subprocess_module.TimeoutExpired:
            logger.error("freesurfer_docker_processing_timeout",
                        subject_id=subject_id,
                        timeout_minutes=FREESURFER_PROCESSING_TIMEOUT_MINUTES)
            raise RuntimeError(f"FreeSurfer Docker processing timed out after {FREESURFER_PROCESSING_TIMEOUT_MINUTES} minutes for {subject_id}")

    # _is_freesurfer_native_available method removed - native FreeSurfer support disabled

    def _check_freesurfer_runtime_availability(self) -> str:
        """Check which container runtime is available for FreeSurfer."""
        # Check Docker availability
        docker_available = self._is_docker_available()

        # Check Singularity availability and FreeSurfer image
        singularity_available = self._is_singularity_available() and self._find_freesurfer_singularity_image() is not None

        if docker_available and singularity_available:
            return "both"
        elif docker_available:
            return "docker"
        elif singularity_available:
            return "singularity"
        else:
            return "none"

    def _extract_freesurfer_hippocampus_data(self, freesurfer_dir: Path, subject_id: str) -> Dict[str, float]:
        """Extract hippocampus volumes from FreeSurfer aseg.stats."""
        stats_file = freesurfer_dir / subject_id / "stats" / "aseg.stats"

        if not stats_file.exists():
            logger.warning("freesurfer_stats_file_missing",
                          path=str(stats_file),
                          subject_id=subject_id)
            return {}

        volumes = {}
        try:
            with open(stats_file, 'r') as f:
                for line in f:
                    if line.strip() and not line.startswith('#'):
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            label_name = parts[4]  # Structure name
                            volume_mm3 = float(parts[3])  # Volume in mm¬≥

                            if "Left-Hippocampus" in label_name:
                                volumes['left_hippocampus'] = volume_mm3
                                logger.info("freesurfer_left_hippocampus_extracted",
                                           volume=volume_mm3, subject_id=subject_id)
                            elif "Right-Hippocampus" in label_name:
                                volumes['right_hippocampus'] = volume_mm3
                                logger.info("freesurfer_right_hippocampus_extracted",
                                           volume=volume_mm3, subject_id=subject_id)

        except Exception as e:
            logger.error("freesurfer_stats_parsing_failed",
                        error=str(e),
                        subject_id=subject_id,
                        stats_file=str(stats_file))
            return {}

        # Validate we got both hemispheres
        if len(volumes) < 2:
            logger.warning("freesurfer_incomplete_hippocampus_data",
                          extracted_volumes=volumes,
                          subject_id=subject_id)

        return volumes

    def _convert_freesurfer_to_fastsurfer_format(self, freesurfer_dir: Path, target_dir: Path) -> Path:
        """Convert FreeSurfer output for downstream processing."""
        subject_id = f"freesurfer_fallback_{self.job_id}"

        # Create output directory structure
        fs_dir = target_dir / str(self.job_id)
        fs_dir.mkdir(exist_ok=True)

        # Create stats directory
        fs_stats_dir = fs_dir / "stats"
        fs_stats_dir.mkdir(exist_ok=True)

        # Copy FreeSurfer stats (aseg.stats contains hippocampus data)
        freesurfer_stats = freesurfer_dir / subject_id / "stats" / "aseg.stats"
        if freesurfer_stats.exists():
            import shutil
            shutil.copy2(freesurfer_stats, fs_stats_dir / "aseg.stats")
            logger.info("freesurfer_stats_copied",
                       from_path=str(freesurfer_stats),
                       to_path=str(fs_stats_dir / "aseg.stats"))

        # Create mock subfield files for testing (FreeSurfer doesn't provide detailed subfields)
        self._create_mock_fastsurfer_subfields_from_hippocampus(fs_dir)

        return target_dir

    def _create_mock_fastsurfer_subfields_from_hippocampus(self, fs_dir: Path) -> None:
        """Create mock subfield files based on FreeSurfer whole hippocampus volumes."""
        # Read FreeSurfer volumes
        aseg_stats = fs_dir / "stats" / "aseg.stats"
        volumes = self._extract_freesurfer_hippocampus_data_from_file(aseg_stats)

        if not volumes:
            logger.warning("no_hippocampus_volumes_for_mock_subfields")
            return

        # Create mock subfield files (approximate distribution based on literature)
        # CA1: ~30%, CA3: ~10%, Subiculum: ~25%, DG: ~20%, etc.
        left_total = volumes.get('left_hippocampus', 0)
        right_total = volumes.get('right_hippocampus', 0)

        if left_total > 0:
            self._write_mock_subfield_stats(fs_dir, "lh", left_total)
        if right_total > 0:
            self._write_mock_subfield_stats(fs_dir, "rh", right_total)

        logger.info("mock_fastsurfer_subfields_created",
                   left_total=left_total,
                   right_total=right_total)

    def _extract_freesurfer_hippocampus_data_from_file(self, stats_file: Path) -> Dict[str, float]:
        """Extract hippocampus volumes directly from aseg.stats file."""
        if not stats_file.exists():
            return {}

        volumes = {}
        try:
            with open(stats_file, 'r') as f:
                for line in f:
                    if line.strip() and not line.startswith('#'):
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            label_name = parts[4]
                            volume_mm3 = float(parts[3])

                            if "Left-Hippocampus" in label_name:
                                volumes['left_hippocampus'] = volume_mm3
                            elif "Right-Hippocampus" in label_name:
                                volumes['right_hippocampus'] = volume_mm3
        except Exception as e:
            logger.error("freesurfer_file_parsing_failed", error=str(e), file=str(stats_file))
            return {}

        return volumes

    def _write_mock_subfield_stats(self, fs_dir: Path, hemisphere: str, total_volume: float) -> None:
        """Write mock subfield statistics file."""
        # Approximate subfield volume distribution (based on literature)
        subfield_ratios = {
            'CA1': 0.30,      # Cornu Ammonis 1
            'CA3': 0.10,      # Cornu Ammonis 3
            'Subiculum': 0.25, # Subiculum
            'DG': 0.20,       # Dentate Gyrus
            'CA4': 0.08,      # Cornu Ammonis 4
            'SRLM': 0.04,     # Stratum radiatum lacunosum moleculare
            'Cyst': 0.03      # Cyst/Other
        }

        stats_content = f"""# Mock subfield stats for FreeSurfer fallback
# Total {hemisphere} hippocampus volume: {total_volume:.2f} mm¬≥
# Generated from FreeSurfer whole hippocampus segmentation

"""

        for subfield, ratio in subfield_ratios.items():
            volume = total_volume * ratio
            stats_content += f"{hemisphere}-{subfield} {volume:.2f} mm¬≥\n"

            # Write to standard output filename
        filename = f"{hemisphere}.hippoSfVolumes-T1.v21.txt"
        stats_file = fs_dir / "stats" / filename

        with open(stats_file, 'w') as f:
            f.write(stats_content)

        logger.info("mock_subfield_stats_written",
                   file=str(stats_file),
                   hemisphere=hemisphere,
                   total_volume=total_volume)

    def _run_freesurfer_singularity(self, nifti_path: Path, output_dir: Path) -> Path:
        """Execute complete FreeSurfer segmentation using Singularity as fallback (autorecon1 + autorecon2-volonly + mri_segstats)."""
        logger.info("starting_freesurfer_singularity_fallback", input=str(nifti_path))

        subject_id = f"freesurfer_fallback_{self.job_id}"
        freesurfer_dir = output_dir / "freesurfer_fallback"
        freesurfer_dir.mkdir(exist_ok=True)

        # Check if FreeSurfer Singularity image is available
        singularity_image = self._find_freesurfer_singularity_image()
        if not singularity_image:
            raise RuntimeError("FreeSurfer Singularity image not found")

        # Get license path
        license_path = self._get_freesurfer_license_path()
        if not license_path:
            raise RuntimeError("FreeSurfer license not found")

        # Update progress
        if self.progress_callback:
            self.progress_callback(
                self._get_current_progress(),
                f"Processing with FreeSurfer (Singularity) ({subject_id})..."
            )

        # IMPORTANT: Clean up any existing subject directory to prevent "re-run existing subject" error
        subject_output_dir = freesurfer_dir / subject_id
        if subject_output_dir.exists():
            logger.warning("freesurfer_subject_dir_exists",
                          path=str(subject_output_dir),
                          message="Removing existing subject directory to allow recon-all -i to run")
            import shutil
            shutil.rmtree(subject_output_dir)

        # Execute with timeout
        try:
            # FreeSurfer Apptainer/Singularity command combining autorecon1 and autorecon2-volonly
            singularity_cmd = [
                "apptainer", "exec",
                "--cleanenv",  # Clean environment
                "-B", f"{freesurfer_dir}:/subjects",
                "-B", f"{nifti_path.parent}:/input:ro",
                "-B", f"{license_path}:/usr/local/freesurfer/license.txt:ro",
                "--env", f"FS_LICENSE=/usr/local/freesurfer/license.txt",
                "--env", f"SUBJECTS_DIR=/subjects",
                str(singularity_image),
                "recon-all",
                "-i", f"/input/{nifti_path.name}",
                "-s", subject_id,
                "-autorecon1",
                "-autorecon2-volonly"  # Combined in single command as requested
            ]

            logger.info("executing_freesurfer_singularity_combined_autorecon",
                       command=" ".join(singularity_cmd),
                       subject_id=subject_id,
                       singularity_image=str(singularity_image),
                       timeout_minutes=FREESURFER_PROCESSING_TIMEOUT_MINUTES)

            # Start progress monitoring
            status_log_path = subject_output_dir / "scripts" / "recon-all-status.log"
            logger.info("starting_progress_monitor", log_path=str(status_log_path), log_exists=status_log_path.exists(), subject_dir=str(subject_output_dir))
            progress_monitor = self._start_freesurfer_progress_monitor(status_log_path, base_progress=self._get_current_progress())

            # Run combined autorecon1 + autorecon2-volonly
            logger.info("freesurfer_singularity_starting_combined_autorecon",
                       command=" ".join(singularity_cmd))

            result = subprocess_module.run(
                singularity_cmd,
                capture_output=True,
                timeout=FREESURFER_PROCESSING_TIMEOUT_MINUTES*60,
                env=self._get_extended_env()
            )

            if result.returncode != 0:
                error_msg = result.stderr.decode()[:500] if result.stderr else "Unknown error"
                logger.error("freesurfer_singularity_combined_autorecon_failed",
                           returncode=result.returncode,
                           error=error_msg)
                raise RuntimeError(f"FreeSurfer Singularity combined autorecon failed: {error_msg}")

            logger.info("freesurfer_singularity_combined_autorecon_completed")

            # Now run mri_segstats to generate aseg.stats from aseg.auto.mgz
            subject_mri_dir = freesurfer_dir / subject_id / "mri"
            subject_stats_dir = freesurfer_dir / subject_id / "stats"
            subject_stats_dir.mkdir(exist_ok=True, parents=True)

            aseg_auto_mgz = subject_mri_dir / "aseg.auto.mgz"
            aseg_stats = subject_stats_dir / "aseg.stats"
            brain_mgz = subject_mri_dir / "brain.mgz"

            if aseg_auto_mgz.exists():
                # Run mri_segstats to generate aseg.stats
                segstats_cmd = [
                    "apptainer", "exec",
                    "--cleanenv",
                    "-B", f"{freesurfer_dir}:/subjects",
                    "-B", f"{license_path}:/usr/local/freesurfer/license.txt:ro",
                    "--env", f"FS_LICENSE=/usr/local/freesurfer/license.txt",
                    "--env", f"SUBJECTS_DIR=/subjects",
                    str(singularity_image),
                    "mri_segstats",
                    "--seg", f"/subjects/{subject_id}/mri/aseg.auto.mgz",
                    "--excludeid", "0",
                    "--sum", f"/subjects/{subject_id}/stats/aseg.stats",
                    "--i", f"/subjects/{subject_id}/mri/brain.mgz"
                ]

                logger.info("running_singularity_mri_segstats_after_combined_autorecon", command=" ".join(segstats_cmd))
                segstats_result = subprocess_module.run(
                    segstats_cmd,
                    capture_output=True,
                    timeout=300,  # 5 minutes for stats generation
                    env=self._get_extended_env()
                )

                if segstats_result.returncode != 0:
                    error_msg = segstats_result.stderr.decode()[:500] if segstats_result.stderr else "Unknown error"
                    logger.warning("singularity_mri_segstats_failed_after_combined_autorecon",
                                 returncode=segstats_result.returncode,
                                 error=error_msg)
                    # Don't raise error - continue with processing, fallback will handle missing stats
                else:
                    logger.info("singularity_mri_segstats_completed_after_combined_autorecon")
            else:
                logger.warning("aseg.auto.mgz_not_found_after_combined_autorecon_in_singularity", path=str(aseg_auto_mgz))

            if result.returncode == 0:
                logger.info("freesurfer_singularity_processing_completed", subject_id=subject_id)
                return freesurfer_dir

            else:
                error_msg = result.stderr.decode()[:500] if result.stderr else "Unknown error"
                logger.error("freesurfer_singularity_autorecon2_failed",
                           returncode=result.returncode,
                           error=error_msg,
                           subject_id=subject_id)
                raise RuntimeError(f"FreeSurfer Singularity autorecon2 failed: {error_msg}")

        except subprocess_module.TimeoutExpired:
            logger.error("freesurfer_singularity_processing_timeout",
                        subject_id=subject_id,
                        timeout_minutes=FREESURFER_PROCESSING_TIMEOUT_MINUTES)
            raise RuntimeError(f"FreeSurfer Singularity processing timed out after {FREESURFER_PROCESSING_TIMEOUT_MINUTES} minutes for {subject_id}")

    def _run_freesurfer_primary(self, nifti_path: Path) -> Path:
        """
        Run FreeSurfer as the primary segmentation method (FreeSurfer-only approach).

        This uses FreeSurfer-only processing for complete brain segmentation.
        """
        logger.info("running_freesurfer_primary_segmentation", input=str(nifti_path))

        freesurfer_dir = self.output_dir / "freesurfer"
        freesurfer_dir.mkdir(exist_ok=True)

        # Smoke test mode: Skip processing and create mock output immediately
        if self.smoke_test_mode:
            logger.info("smoke_test_mode_enabled", message="Using mock FreeSurfer data for CI testing")
            # For FreeSurfer-only, we'll create FreeSurfer-compatible mock data
            self._create_mock_freesurfer_output(freesurfer_dir)
            return freesurfer_dir

        # Check FreeSurfer availability and license before processing
        logger.info("checking_freesurfer_requirements")

        # Check for FreeSurfer license first
        license_path = self._get_freesurfer_license_path()
        if not license_path:
            error_msg = ("FreeSurfer license not found. Please place your FreeSurfer license.txt file in the same folder as the NeuroInsight application, or set the FREESURFER_LICENSE environment variable to point to your license file. "
                        "You can obtain a FreeSurfer license from: https://surfer.nmr.mgh.harvard.edu/registration.html")
            logger.error("freesurfer_license_not_found", error=error_msg)
            raise RuntimeError(f"FreeSurfer license required: {error_msg}")

        # Priority order: Docker (main) ‚Üí Apptainer (fallback) ‚Üí Mock data (final fallback)

        # Check if Docker is available (primary choice for most users)
        if self._is_docker_available():
            logger.info("freesurfer_docker_available_using_as_primary")
            try:
                # Use Docker container as primary method
                return self._run_freesurfer_docker(nifti_path, freesurfer_dir, license_path)
            except Exception as docker_error:
                logger.warning("freesurfer_docker_failed_falling_back", error=str(docker_error))

        # Check if Apptainer/Singularity containers are available (fallback for HPC/specialized systems)
        sif_path = self._find_freesurfer_sif()
        if sif_path:
            logger.info("freesurfer_apptainer_available_using_as_fallback", sif_path=str(sif_path))
            try:
                # Use Apptainer/Singularity container as fallback
                subject_id = f"freesurfer_apptainer_{self.job_id}"
                return self._run_freesurfer_singularity_local(nifti_path, freesurfer_dir, license_path, sif_path)
            except Exception as apptainer_error:
                logger.error("freesurfer_apptainer_failed_falling_back",
                           error=str(apptainer_error),
                           error_type=type(apptainer_error).__name__,
                           sif_path=str(sif_path),
                           nifti_path=str(nifti_path),
                           freesurfer_dir=str(freesurfer_dir))
                import traceback
                logger.error("freesurfer_apptainer_traceback",
                           traceback=traceback.format_exc())

        # No container runtimes available - use mock data
        logger.warning("no_freesurfer_containers_available_using_mock_data")
        self._create_mock_freesurfer_output(freesurfer_dir)
        return freesurfer_dir

        logger.info("freesurfer_requirements_met", license_path=str(license_path))

        # Run FreeSurfer segmentation directly
        logger.info("starting_freesurfer_primary_processing")

        try:
            # Use the existing FreeSurfer fallback method as primary
            result_dir = self._run_freesurfer_fallback(nifti_path, freesurfer_dir.parent)

            # Convert to consistent output format
            self._convert_freesurfer_to_fastsurfer_format(result_dir, freesurfer_dir)

            logger.info("freesurfer_primary_processing_successful")
            return freesurfer_dir

        except Exception as freesurfer_error:
            logger.error("freesurfer_primary_processing_failed",
                        error=str(freesurfer_error),
                        error_type=type(freesurfer_error).__name__)

            # Single fallback: mock data
            logger.warning("freesurfer_failed_using_mock_data")
            self._create_mock_freesurfer_output(freesurfer_dir)
            return freesurfer_dir

    def _create_mock_freesurfer_output(self, output_dir: Path) -> None:
        """
        Create mock FreeSurfer output for development/testing (FreeSurfer-only mode).

        Generates FreeSurfer directory structure and stats files.
        """
        logger.info("creating_mock_freesurfer_output", output_dir=str(output_dir))

        subject_id = f"mock_freesurfer_{self.job_id}"

        # Create FreeSurfer directory structure
        subject_dir = output_dir / subject_id
        subject_dir.mkdir(exist_ok=True)

        stats_dir = subject_dir / "stats"
        stats_dir.mkdir(exist_ok=True)

        # Create mock aseg.stats with realistic hippocampal volumes
        aseg_stats_path = stats_dir / "aseg.stats"

        mock_stats = """# Title Segmentation Statistics
#
# subjectname mock_freesurfer
# subjectsdir /output
# txtime 2024-01-01 12:00:00
# etime 90.2
# nc 2
# nv 1000000
# atlas_icv 1500000.0
# possible_wm 800000.0
# cmdargs -i input.nii -s mock_freesurfer -autorecon1 -autorecon2-volonly

# ColHeaders Index SegId NVoxels Volume_mm3 StructName normMean normStdDev normMin normMax normRange
  1     17     2800   2800.0  Left-Hippocampus    45.6   12.3    20.1    89.2    69.1
  2     53     2750   2750.0  Right-Hippocampus   47.2   11.8    21.5    91.1    69.6
"""

        with open(aseg_stats_path, 'w') as f:
            f.write(mock_stats)

        logger.info("mock_freesurfer_output_created", subject_id=subject_id, stats_file=str(aseg_stats_path))

    def _find_freesurfer_singularity_image(self) -> Path:
        """Find the FreeSurfer Singularity image."""
        # Check configured path first
        if hasattr(settings, 'freesurfer_singularity_image_path'):
            img_path = Path(settings.freesurfer_singularity_image_path)
            if img_path.exists():
                logger.info("found_configured_freesurfer_singularity_image", path=str(img_path))
                return img_path

        # Check current working directory containers first (for development)
        cwd_container = Path.cwd() / "containers" / "freesurfer.sif"
        if cwd_container.exists():
            logger.info("found_cwd_freesurfer_singularity_image", path=str(cwd_container))
            return cwd_container

        # Check app-bundled containers directory (for deployed apps)
        try:
            app_root = self._get_app_root_directory()
            app_container = app_root / "containers" / "freesurfer.sif"
            if app_container.exists():
                logger.info("found_app_bundled_freesurfer_singularity_image", path=str(app_container))
                return app_container
        except:
            pass  # Ignore app root detection failures

        # Check common locations for FreeSurfer Singularity images
        common_locations = [
            # User-specific locations
            Path.home() / "Documents/containers/freesurfer_latest.sif",
            Path.home() / "containers/freesurfer_latest.sif",
            Path.home() / ".singularity/cache/freesurfer_latest.sif",

            # Shared/HPC locations
            Path("/shared/containers/freesurfer_latest.sif"),
            Path("/opt/containers/freesurfer_latest.sif"),
            Path("/opt/hpc/containers/freesurfer_latest.sif"),
            Path("/cm/shared/containers/freesurfer_latest.sif"),
            Path("/scratch/containers/freesurfer_latest.sif"),
            Path("/project/containers/freesurfer_latest.sif"),

            # System-wide locations
            Path("/usr/local/singularity/images/freesurfer_latest.sif"),
            Path("/opt/singularity/images/freesurfer_latest.sif"),
        ]

        for img_path in common_locations:
            if img_path.exists():
                logger.info("found_freesurfer_singularity_image", path=str(img_path))
                return img_path

        # No existing images found - try automatic download
        logger.info("no_freesurfer_singularity_images_found_attempting_download")
        downloaded_image = self._ensure_singularity_container()
        if downloaded_image and downloaded_image.exists():
            logger.info("freesurfer_singularity_download_successful", path=str(downloaded_image))
            return downloaded_image

        logger.warning("freesurfer_singularity_image_not_found_and_download_failed")
        return None

    def _find_singularity_image(self) -> Path:
        """
        Find the FreeSurfer Singularity image.

        Returns:
            Path to the Singularity image if found, None otherwise
        """
        # Check configured path first
        if hasattr(settings, 'singularity_image_path'):
            img_path = Path(settings.singularity_image_path)
            if img_path.exists():
                logger.info("found_configured_singularity_image", path=str(img_path))
                return img_path

        # Check common locations for Singularity images
        common_locations = [
            # User-specific locations
            Path.home() / "Documents/containers/fastsurfer_latest.sif",
            Path.home() / "containers/fastsurfer_latest.sif",
            Path.home() / ".singularity/cache/fastsurfer_latest.sif",

            # Shared/HPC locations
            Path("/shared/containers/fastsurfer_latest.sif"),
            Path("/opt/containers/fastsurfer_latest.sif"),
            Path("/opt/hpc/containers/fastsurfer_latest.sif"),
            Path("/cm/shared/containers/fastsurfer_latest.sif"),
            Path("/scratch/containers/fastsurfer_latest.sif"),
            Path("/project/containers/fastsurfer_latest.sif"),

            # System-wide locations
            Path("/usr/local/share/singularity/images/fastsurfer_latest.sif"),
            Path("/var/lib/singularity/images/fastsurfer_latest.sif"),

            # Alternative naming patterns
            Path("/shared/containers/fastsurfer.sif"),
            Path("/opt/containers/fastsurfer.sif"),
        ]

        for location in common_locations:
            if location.exists():
                logger.info("found_singularity_image", path=str(location))
                return location

        logger.info("singularity_image_not_found")
        return None


    def _run_fastsurfer(self, nifti_path: Path) -> Path:
        """
        Run FreeSurfer segmentation using Docker.

        Executes FreeSurfer container for whole brain segmentation.
        In smoke test mode, immediately returns mock data for faster CI testing.

        Args:
            nifti_path: Path to input NIfTI file

        Returns:
            Path to FreeSurfer output directory

        Raises:
            DockerNotAvailableError: If Docker is not installed or not running
        """
        logger.info("running_fastsurfer_docker", input=str(nifti_path))

        freesurfer_dir = self.output_dir / "fastsurfer"
        freesurfer_dir.mkdir(exist_ok=True)

        # Smoke test mode: Skip Docker and create mock output immediately
        if self.smoke_test_mode:
            logger.info("smoke_test_mode_enabled", message="Using mock FreeSurfer data for CI testing")
            self._create_mock_fastsurfer_output(freesurfer_dir)
            return freesurfer_dir

        # Smart Container Runtime Selection with Automatic Fallback
        # Strategy: Try preferred runtime first, fallback to alternatives if available

        container_runtime = self._check_container_runtime_availability()
        attempted_runtimes = []

        # Primary attempt with selected runtime
        if container_runtime == "docker":
            logger.info("attempting_docker_as_primary_runtime")
            attempted_runtimes.append("docker")
            try:
                return self._run_fastsurfer_docker(nifti_path, freesurfer_dir)
            except Exception as docker_error:
                logger.warning("docker_execution_failed", error=str(docker_error), error_type=type(docker_error).__name__)
                # Docker failed - try Singularity fallback if available
                singularity_available = self._is_singularity_available()
                logger.info("checking_singularity_for_fallback", available=singularity_available)
                if singularity_available:
                    logger.info("docker_failed_trying_singularity_fallback")
                    attempted_runtimes.append("singularity")
                    try:
                        return self._run_fastsurfer_singularity(nifti_path, freesurfer_dir)
                    except Exception as sing_error:
                        logger.warning("singularity_fallback_failed", error=str(sing_error), error_type=type(sing_error).__name__)
                else:
                    logger.warning("singularity_not_available_for_fallback")

        elif container_runtime == "singularity":
            logger.info("attempting_singularity_as_primary_runtime")
            attempted_runtimes.append("singularity")
            try:
                return self._run_fastsurfer_singularity(nifti_path, freesurfer_dir)
            except Exception as sing_error:
                logger.warning("singularity_execution_failed", error=str(sing_error))
                # Singularity failed - try Docker fallback if available
                if self._is_docker_available():
                    logger.info("singularity_failed_trying_docker_fallback")
                    attempted_runtimes.append("docker")
                    try:
                        return self._run_fastsurfer_docker(nifti_path, freesurfer_dir)
                    except Exception as docker_error:
                        logger.warning("docker_fallback_failed", error=str(docker_error))

        # ===== FREESURFER FALLBACK =====
        # If container execution failed, fall back to mock data
        if self._is_freesurfer_available():
            logger.info("fastsurfer_failed_trying_freesurfer_fallback")
            try:
                freesurfer_result_dir = self._run_freesurfer_fallback(nifti_path, freesurfer_dir.parent)

                # Use FreeSurfer output directly
                compatible_dir = self._convert_freesurfer_to_fastsurfer_format(
                    freesurfer_result_dir, freesurfer_dir
                )

                logger.info("freesurfer_fallback_successful")
                return compatible_dir

            except Exception as freesurfer_error:
                logger.error("freesurfer_fallback_failed",
                           fastsurfer_error=str(freesurfer_error),
                           freesurfer_error=str(freesurfer_error))

        # Final fallback: mock data
        logger.warning("all_segmentation_methods_failed_using_mock_data",
                      attempted_runtimes=attempted_runtimes)
        self._create_mock_fastsurfer_output(freesurfer_dir)
        return freesurfer_dir

    def _run_fastsurfer_docker(self, nifti_path: Path, freesurfer_dir: Path) -> Path:
        """
        Run FreeSurfer using Docker.

        Args:
            nifti_path: Path to input NIfTI file
            freesurfer_dir: Output directory

        Returns:
            Path to FreeSurfer output directory
        """
        try:
            result = subprocess_module.run(
                ["docker", "images", "-q", "deepmi/fastsurfer:latest"],
                capture_output=True,
                timeout=10
            )
            if not result.stdout.strip():
                # Image not found - need to download
                logger.info("fastsurfer_image_not_found", message="Will download FastSurfer image")
                if self.progress_callback:
                    self.progress_callback(
                        15,
                        "Downloading FreeSurfer container (4GB, first time only - takes 10-15 min)..."
                    )
                
                # Pull the image
                logger.info("pulling_fastsurfer_image")
                # Use the same extended PATH environment for consistency
                env = os.environ.copy()
                current_path = env.get('PATH', '')
                # Add common Docker locations to PATH
                import getpass
                user_home = os.path.expanduser('~')
                docker_paths = [
                    f'{user_home}/bin',  # User's bin directory
                    '/usr/local/bin',     # Common alternative location
                    '/usr/bin',           # System default
                    '/bin',               # Fallback
                    '/opt/bin',           # Optional packages
                    '/snap/bin',          # Snap packages
                ]
                extended_path = current_path
                for path in docker_paths:
                    if path not in current_path:
                        extended_path = f"{path}:{extended_path}"
                env['PATH'] = extended_path

                # Use enhanced Docker download with progress messages
                try:
                    self._download_docker_image_with_progress(
                        "deepmi/fastsurfer:latest",
                        "FastSurfer",
                        env
                    )
                except subprocess_module.CalledProcessError as e:
                    logger.error("fastsurfer_pull_failed", error=str(e))
                    raise RuntimeError(
                        f"Failed to download FreeSurfer container: {str(e)}\n\n"
                        "Please check your internet connection and try again."
                    )
                
                logger.info("fastsurfer_image_downloaded", message="FastSurfer model ready")
        except subprocess_module.TimeoutExpired:
            raise RuntimeError(
                "Downloading FreeSurfer container timed out. "
                "Please check your internet connection and try again."
            )
        
        try:
            # Use CPU processing
            device = "cpu"
            runtime_arg = ""

            # Determine optimal thread count for CPU processing
            import os
            cpu_count = os.cpu_count() or 4
            num_threads = max(1, cpu_count - 2)  # Leave 2 cores free
            
            # Get host paths for Docker-in-Docker mounting
            # When worker runs inside Docker and spawns FastSurfer Docker container,
            # we need to mount HOST paths, not container paths
            host_upload_dir = os.getenv('HOST_UPLOAD_DIR')
            host_output_dir = os.getenv('HOST_OUTPUT_DIR')
            
            # If not set, try to auto-detect from Docker inspect
            if not host_upload_dir or not host_output_dir:
                try:
                    import json
                    
                    # Get our own container info
                    # Use the same extended PATH environment for consistency
                    env = os.environ.copy()
                    current_path = env.get('PATH', '')
                    # Add common Docker locations to PATH
                    import getpass
                    user_home = os.path.expanduser('~')
                    docker_paths = [
                        f'{user_home}/bin',  # User's bin directory
                        '/usr/local/bin',     # Common alternative location
                        '/usr/bin',           # System default
                        '/bin',               # Fallback
                        '/opt/bin',           # Optional packages
                        '/snap/bin',          # Snap packages
                    ]
                    extended_path = current_path
                    for path in docker_paths:
                        if path not in current_path:
                            extended_path = f"{path}:{extended_path}"
                    env['PATH'] = extended_path

                    result = subprocess_module.run(
                        ['docker', 'inspect', os.uname().nodename],
                        capture_output=True,
                        text=True,
                        check=True,
                        env=env
                    )
                    container_info = json.loads(result.stdout)[0]
                    
                    # Extract mount sources from our container
                    for mount in container_info.get('Mounts', []):
                        dest = mount.get('Destination', '')
                        if dest == '/data/uploads' and not host_upload_dir:
                            host_upload_dir = mount.get('Source')
                        elif dest == '/data/outputs' and not host_output_dir:
                            host_output_dir = mount.get('Source')
                    
                    logger.info(
                        "auto_detected_host_paths",
                        upload_dir=host_upload_dir,
                        output_dir=host_output_dir
                    )
                except Exception as e:
                    logger.warning(
                        "host_path_detection_failed",
                        error=str(e),
                        note="Falling back to configured desktop paths"
                    )
                    host_upload_dir = host_upload_dir or ''
                    host_output_dir = host_output_dir or ''
            else:
                logger.info(
                    "using_configured_host_paths",
                    upload_dir=host_upload_dir,
                    output_dir=host_output_dir
                )

            # If host paths are still unset or point to placeholder mount locations,
            # fall back to the actual desktop storage directories.
            if not host_upload_dir or not Path(host_upload_dir).exists() or host_upload_dir == "/data/uploads":
                host_upload_dir = str(Path(settings.upload_dir).resolve())

            if not host_output_dir or not Path(host_output_dir).exists() or host_output_dir == "/data/outputs":
                host_output_dir = str(Path(settings.output_dir).resolve())

            logger.info(
                "resolved_host_paths",
                upload_dir=host_upload_dir,
                output_dir=host_output_dir
            )
            
            # Calculate relative paths from host perspective
            # nifti_path is like /data/uploads/file.nii (inside worker container)
            # We need to translate to host path
            input_host_path = host_upload_dir
            output_host_path = f"{host_output_dir}/{self.job_id}/fastsurfer"
            
            # Build Docker command
            cmd = ["docker", "run", "--rm"]
            
            allow_root = False
            force_root = os.getenv("FASTSURFER_FORCE_ROOT") == "1"
            # Add GPU support if available
            if runtime_arg:
                cmd.extend(runtime_arg.split())
            
            # On Windows or when force_root is set, FastSurfer image defaults to user "nonroot"
            # which cannot read NTFS-mounted paths. Override to root and allow root exec.
            force_root_reason = None
            if platform.system() == "Windows" or force_root:
                cmd.extend(["--user", "root"])
                allow_root = True
                force_root_reason = (
                    "windows_platform" if platform.system() == "Windows" else "forced_by_env"
                )
            
            # Add volume mounts with HOST paths
            cmd.extend([
                "-v", f"{input_host_path}:/input:ro",
                "-v", f"{output_host_path}:/output",
                "deepmi/fastsurfer:latest",
                "--t1", f"/input/{nifti_path.name}",
                "--sid", str(self.job_id),
                "--sd", "/output",
                "--seg_only",  # Only segmentation, skip surface reconstruction
                "--device", device,
                "--batch", "1",
                "--threads", str(num_threads),
                "--viewagg_device", "cpu",
            ])

            if allow_root:
                logger.info(
                    "forcing_root_user_for_fastsurfer",
                    reason=force_root_reason,
                    platform=platform.system(),
                )
                cmd.append("--allow_root")
            
            if device == "cpu":
                logger.info(
                    "cpu_threading_enabled",
                    threads=num_threads,
                    total_cores=cpu_count,
                    note=f"Using {num_threads} threads for CPU processing"
                )
            
            logger.info(
                "executing_fastsurfer",
                command=" ".join(cmd),
                note="Running FastSurfer with Docker"
            )
            
            # Use the same extended PATH environment for consistency
            env = os.environ.copy()
            current_path = env.get('PATH', '')
            # Add common Docker locations to PATH
            import getpass
            user_home = os.path.expanduser('~')
            docker_paths = [
                f'{user_home}/bin',       # User's bin directory (most common)
                '/usr/local/bin',         # Manual installs, Homebrew (Linux)
                '/usr/bin',               # System default (apt, dnf, pacman, etc.)
                '/bin',                   # Fallback system path
                '/opt/bin',               # Optional packages
                '/snap/bin',              # Snap packages
                '/opt/docker-desktop/bin', # Docker Desktop for Linux
                '/opt/docker/bin',        # Alternative Docker installs
            ]
            extended_path = current_path
            for path in docker_paths:
                if path not in current_path:
                    extended_path = f"{path}:{extended_path}"
            env['PATH'] = extended_path

            result = subprocess_module.run(
                cmd,
                check=True,
                capture_output=True,
                text=True,
                timeout=settings.processing_timeout,
                env=env
            )
            
            logger.info(
                "fastsurfer_completed",
                output_dir=str(freesurfer_dir),
                note="Brain segmentation complete"
            )
            
        except subprocess_module.TimeoutExpired:
            logger.error("fastsurfer_timeout")
            logger.warning(
                "using_mock_data",
                reason="Processing timeout - using mock data"
            )
            self._create_mock_fastsurfer_output(freesurfer_dir)
        
        except subprocess_module.CalledProcessError as e:
            # Docker command failed - comprehensive error checking for Singularity fallback
            stderr_lower = (e.stderr or "").lower() if hasattr(e, 'stderr') else ""
            stdout_lower = (e.stdout or "").lower() if hasattr(e, 'stdout') else ""

            # Check for various Docker failure modes that should trigger Singularity fallback
            docker_failure_indicators = [
                "cannot connect to the docker daemon",
                "docker daemon is not running",
                "permission denied",
                "no such file or directory",
                "docker: command not found",
                "cannot apply additional memory protection",
                "error while loading shared libraries",
                "connection refused",
                "timeout",
            ]

            should_try_singularity = any(indicator in stderr_lower or indicator in stdout_lower
                                       for indicator in docker_failure_indicators)

            if should_try_singularity:
                logger.warning(
                    "docker_failed_trying_singularity",
                    error=str(e),
                    stderr=e.stderr if hasattr(e, 'stderr') else "No stderr",
                    returncode=e.returncode,
                )
                try:
                    return self._run_fastsurfer_singularity(nifti_path, freesurfer_dir)
                except Exception as sing_error:
                    logger.warning(
                        "singularity_fallback_failed",
                        docker_error=str(e),
                        singularity_error=str(sing_error),
                        note="Using mock data as final fallback"
                    )
                    self._create_mock_fastsurfer_output(freesurfer_dir)
                    return freesurfer_dir

            # Docker command succeeded but returned error - log and try Singularity anyway
            logger.error(
                "fastsurfer_execution_failed_trying_singularity",
                error=str(e),
                stderr=e.stderr if hasattr(e, 'stderr') and e.stderr else "No stderr",
                stdout=e.stdout if hasattr(e, 'stdout') and e.stdout else "No stdout",
                returncode=e.returncode,
            )

            # Try Singularity as fallback for any Docker failure
            logger.info("trying_singularity_fallback_after_docker_failure")
            try:
                return self._run_fastsurfer_singularity(nifti_path, freesurfer_dir)
            except Exception as sing_error:
                logger.warning(
                    "singularity_fallback_failed",
                    error=str(sing_error),
                    note="Using mock data as final fallback"
                )
                self._create_mock_fastsurfer_output(freesurfer_dir)
        
        except Exception as e:
            logger.error(
                "fastsurfer_unexpected_error",
                error=str(e),
                error_type=type(e).__name__
            )
            logger.warning("using_mock_data", reason=f"Unexpected error: {str(e)}")
            self._create_mock_fastsurfer_output(freesurfer_dir)
        
        return freesurfer_dir
    
    def _run_fastsurfer_docker(self, nifti_path: Path, freesurfer_dir: Path) -> Path:
        """
        Run FreeSurfer using Docker.

        Args:
            nifti_path: Path to input NIfTI file
            freesurfer_dir: Output directory

        Returns:
            Path to FreeSurfer output directory

        Raises:
            RuntimeError: If Docker execution fails
        """
        logger.info("running_fastsurfer_docker", input=str(nifti_path))

        # Check if FastSurfer image is downloaded
        try:
            result = subprocess_module.run(
                ["docker", "images", "-q", "deepmi/fastsurfer:latest"],
                capture_output=True,
                timeout=10
            )
            if not result.stdout.strip():
                # Image not found - need to download
                logger.info("fastsurfer_image_not_found", message="Will download FastSurfer image")
                if self.progress_callback:
                    self.progress_callback(
                        15,
                        "Downloading FreeSurfer container (4GB, first time only - takes 10-15 min)..."
                    )

                # Pull the image
                logger.info("pulling_fastsurfer_image")
                env = os.environ.copy()
                current_path = env.get('PATH', '')
                extended_path = f"/usr/local/bin:/usr/bin:/bin:{current_path}"
                env['PATH'] = extended_path

                try:
                    # Use enhanced Docker download with progress messages
                    self._download_docker_image_with_progress(
                        "deepmi/fastsurfer:latest",
                        "FastSurfer",
                        env
                    )
                    logger.info("fastsurfer_image_downloaded_successfully")
                except subprocess_module.CalledProcessError as e:
                    logger.error("fastsurfer_image_download_failed", error=str(e))
                    raise RuntimeError(f"Failed to download FreeSurfer container: {str(e)}")
                except subprocess_module.TimeoutExpired:
                    logger.error("fastsurfer_image_download_timeout")
                    raise RuntimeError("FreeSurfer container download timed out")
            else:
                logger.info("fastsurfer_image_already_present")
        except subprocess_module.TimeoutExpired:
            logger.error("docker_image_check_timeout")
            raise RuntimeError("Docker image check timed out")
        except subprocess_module.CalledProcessError as e:
            logger.error("docker_image_check_failed", error=str(e))
            raise RuntimeError(f"Docker image check failed: {e}")

        # Use CPU processing
        device = "cpu"

        # Threading
        cpu_count = os.cpu_count() or 4
        num_threads = max(1, cpu_count - 2)

        # Build Docker command
        # Use absolute paths for Docker volume mounts
        abs_input_dir = nifti_path.parent.resolve()
        abs_freesurfer_dir = freesurfer_dir.resolve()
        
        cmd = [
            "docker", "run", "--rm",
            "-v", f"{abs_input_dir}:/input:ro",
            "-v", f"{abs_freesurfer_dir}:/output",
        ]

        cmd.extend([
            "deepmi/fastsurfer:latest",
            "--t1", f"/input/{nifti_path.name}",
            "--sid", str(self.job_id),
            "--sd", "/output",
            "--seg_only",
            "--device", device,
            "--batch", "1",
            "--threads", str(num_threads),
            "--viewagg_device", "cpu",
        ])

        logger.info("cpu_threading_enabled",
                   threads=num_threads,
                   total_cores=cpu_count,
                   note=f"Using {num_threads} threads for CPU processing")

        logger.info("executing_fastsurfer_docker",
                   command=" ".join(cmd),
                   note="Running FastSurfer with Docker")

        # Execute Docker
        env = os.environ.copy()
        current_path = env.get('PATH', '')
        extended_path = f"/usr/local/bin:/usr/bin:/bin:{current_path}"
        env['PATH'] = extended_path

        try:
            result = subprocess_module.run(
                cmd,
                capture_output=True,
                timeout=self.docker_timeout,
                env=env
            )

            if result.returncode == 0:
                logger.info("fastsurfer_docker_completed", output_dir=str(freesurfer_dir))
                return freesurfer_dir
            else:
                logger.error("fastsurfer_docker_failed",
                           returncode=result.returncode,
                           stderr=result.stderr.decode()[:500] if result.stderr else "No stderr",
                           stdout=result.stdout.decode()[:500] if result.stdout else "No stdout")
                raise RuntimeError(f"FastSurfer Docker failed: {result.stderr.decode()}")
        except subprocess_module.TimeoutExpired:
            logger.error("fastsurfer_docker_timeout", timeout=self.docker_timeout)
            raise RuntimeError(f"FastSurfer Docker execution timed out after {self.docker_timeout} seconds")

    def _run_fastsurfer_singularity(self, nifti_path: Path, freesurfer_dir: Path) -> Path:
        """
        Run FreeSurfer using Singularity/Apptainer (fallback when Docker not available).
        
        Args:
            nifti_path: Path to input NIfTI file
            freesurfer_dir: Output directory
            
        Returns:
            Path to FreeSurfer output directory
        """
        import shutil
        import os
        import signal
        
        logger.info("running_fastsurfer_singularity", input=str(nifti_path))
        
        # Check for Singularity/Apptainer with extended PATH
        singularity_cmd = None
        env = os.environ.copy()
        current_path = env.get('PATH', '')

        # Add common Singularity/Apptainer locations to PATH
        user_home = os.path.expanduser('~')
        singularity_paths = [
            f'{user_home}/bin',
            '/usr/local/bin',
            '/usr/bin',
            '/bin',
            '/opt/bin',
            '/opt/singularity/bin',
            '/opt/apptainer/bin',
            '/usr/local/singularity/bin',
            '/usr/local/apptainer/bin',
            '/opt/modulefiles/bin',
            '/cm/shared/apps/singularity',
            '/shared/apps/singularity',
            '/opt/apps/singularity',
            '/opt/hpc/singularity',
        ]

        # Add configured Singularity bin path if specified
        if hasattr(settings, 'singularity_bin_path') and settings.singularity_bin_path:
            singularity_paths.insert(0, settings.singularity_bin_path)
            logger.info("added_configured_singularity_path", path=settings.singularity_bin_path)

        extended_path = current_path
        for path in singularity_paths:
            if path not in current_path:
                extended_path = f"{path}:{extended_path}"
        env['PATH'] = extended_path

        # Check for commands with extended PATH
        def check_command_with_path(cmd):
            try:
                result = subprocess_module.run(
                    ['which', cmd],
                    capture_output=True,
                    text=True,
                    env=env,
                    timeout=5
                )
                if result.returncode == 0 and result.stdout.strip():
                    return True
            except (subprocess_module.TimeoutExpired, subprocess_module.CalledProcessError):
                pass
            return False

        if check_command_with_path("apptainer"):
            singularity_cmd = "apptainer"
        elif check_command_with_path("singularity"):
            singularity_cmd = "singularity"
        else:
            raise FileNotFoundError("Neither singularity nor apptainer found in PATH")
        
        # Find Singularity image
        singularity_img = Path(settings.singularity_image_path) if hasattr(settings, 'singularity_image_path') else None
        if not singularity_img or not singularity_img.exists():
            # Try common locations including HPC paths
            possible_paths = [
                # Project-specific location
                Path("/mnt/nfs/home/urmc-sh.rochester.edu/pndagiji/hippo/singularity-images/fastsurfer.sif"),

                # Relative to output directory
                Path(settings.output_dir).parent / "singularity-images" / "fastsurfer.sif",
                Path("./singularity-images/fastsurfer.sif"),

                # HPC/Shared locations
                Path("/shared/containers/fastsurfer_latest.sif"),
                Path("/opt/containers/fastsurfer_latest.sif"),
                Path("/opt/hpc/containers/fastsurfer_latest.sif"),
                Path("/cm/shared/containers/fastsurfer_latest.sif"),
                Path("/scratch/containers/fastsurfer_latest.sif"),
                Path("/project/containers/fastsurfer_latest.sif"),

                # Alternative naming
                Path("/shared/containers/fastsurfer.sif"),
                Path("/opt/containers/fastsurfer.sif"),
            ]
            for path in possible_paths:
                if path.exists():
                    singularity_img = path
                    break
        
        if not singularity_img or not singularity_img.exists():
            raise FileNotFoundError(f"FastSurfer Singularity image not found")
        
        logger.info("found_singularity_image", path=str(singularity_img))
        
        # Use CPU processing
        device = "cpu"

        # Threading
        cpu_count = os.cpu_count() or 4
        num_threads = max(1, cpu_count - 2)

        # Build Singularity command
        cmd = [singularity_cmd, "exec"]
        
        # Add bind mounts and environment
        cmd.extend([
            "--bind", f"{nifti_path.parent}:/input:ro",
            "--bind", f"{freesurfer_dir}:/output",
            "--env", "TQDM_DISABLE=1",
            "--cleanenv",
            str(singularity_img),
            "/fastsurfer/run_fastsurfer.sh",
            "--t1", f"/input/{nifti_path.name}",
            "--sid", str(self.job_id),
            "--sd", "/output",
            "--seg_only",
            "--device", device,
            "--batch", "1",
            "--threads", str(num_threads),
            "--viewagg_device", "cpu",
        ])
        
        logger.info(
            "cpu_threading_enabled",
            threads=num_threads,
            total_cores=cpu_count,
            note=f"Using {num_threads} threads for CPU parallel processing"
        )
        
        logger.info(
            "executing_fastsurfer_singularity",
            command=" ".join(cmd),
            note="Running FastSurfer with Singularity"
        )
        
        # Execute Singularity with proper process group management
        # Using Popen instead of run() to track PID and manage process group
        process = None
        try:
            # Create a new process group so we can kill all child processes
            # Use extended environment with Singularity PATH
            process = subprocess_module.Popen(
                cmd,
                stdout=subprocess_module.PIPE,
                stderr=subprocess_module.PIPE,
                text=True,
                env=env,  # Use environment with extended PATH for Singularity
                preexec_fn=os.setsid,  # Create new process group
            )
            
            # Store the process PID for cleanup tracking
            self._store_process_pid(process.pid)
            logger.info("process_started", pid=process.pid, pgid=os.getpgid(process.pid))
            
            # Wait for process with timeout
            try:
                stdout, stderr = process.communicate(timeout=7200)
                returncode = process.returncode
            except subprocess_module.TimeoutExpired:
                logger.warning("process_timeout_killing_group", pid=process.pid)
                # Kill entire process group
                try:
                    os.killpg(os.getpgid(process.pid), signal.SIGTERM)
                    process.wait(timeout=10)
                except:
                    os.killpg(os.getpgid(process.pid), signal.SIGKILL)
                raise
            finally:
                self._clear_process_pid()
            
            if returncode != 0:
                logger.error(
                    "fastsurfer_singularity_failed",
                    returncode=returncode,
                    stderr=stderr[:500] if stderr else "No stderr",
                    stdout=stdout[:500] if stdout else "No stdout"
                )
                raise RuntimeError(f"FastSurfer Singularity failed: {stderr}")
            
            logger.info("fastsurfer_singularity_completed", output_dir=str(freesurfer_dir))
            return freesurfer_dir
            
        except Exception as e:
            # Ensure cleanup of process group on any error
            if process and process.poll() is None:
                logger.warning("cleaning_up_process_group_on_error", pid=process.pid)
                try:
                    os.killpg(os.getpgid(process.pid), signal.SIGKILL)
                except:
                    pass
            self._clear_process_pid()
            raise
    
    def _create_mock_fastsurfer_output(self, output_dir: Path) -> None:
        """
        Create mock FreeSurfer output for development/testing.

        Args:
            output_dir: Output directory for mock data
        """
        print("DEBUG: _create_mock_fastsurfer_output method called")
        print(f"DEBUG: creating_mock_fastsurfer_output: {output_dir}")
        try:
            print("DEBUG: try_block_started")
            # Create directory structure for output
            subject_dir = output_dir / str(self.job_id)
            print(f"DEBUG: subject_dir_created: {subject_dir}")
            stats_dir = subject_dir / "stats"
            mri_dir = subject_dir / "mri"
            print("DEBUG: about_to_mkdir")
            stats_dir.mkdir(parents=True, exist_ok=True)
            mri_dir.mkdir(parents=True, exist_ok=True)
            print(f"DEBUG: directories_created: {stats_dir}")

            # Create mock hippocampal subfields stats file
            mock_data = {
                "CA1": {"left": 1250.5, "right": 1198.2},
                "CA3": {"left": 450.3, "right": 465.1},
                "subiculum": {"left": 580.7, "right": 555.9},
                "dentate_gyrus": {"left": 380.2, "right": 395.6},
            }

            # Write left hemisphere stats
            left_stats = stats_dir / "lh.hippoSfVolumes-T1.v21.txt"
            with open(left_stats, "w") as f:
                f.write("# Hippocampal subfield volumes (left hemisphere)\n")
                f.write("# Region Volume\n")
                for region, volumes in mock_data.items():
                    f.write(f"{region} {volumes['left']:.2f}\n")

            # Write right hemisphere stats
            right_stats = stats_dir / "rh.hippoSfVolumes-T1.v21.txt"
            with open(right_stats, "w") as f:
                f.write("# Hippocampal subfield volumes (right hemisphere)\n")
                f.write("# Region Volume\n")
                for region, volumes in mock_data.items():
                    f.write(f"{region} {volumes['right']:.2f}\n")
            logger.info("hippo_stats_created")

            # Create mock aseg stats file
            aseg_stats = stats_dir / "aseg+DKT.stats"
            logger.info(f"creating_aseg_stats: {aseg_stats}")
            try:
                with open(aseg_stats, "w") as f:
                    f.write("# aseg+DKT.stats\n")
                    f.write("# Index SegId NVoxels Volume_mm3 StructName normMean normStdDev normMin normMax normRange\n")
                    f.write("17 17 31262 1250.5 Left-Hippocampus 110.5 15.2 85.3 145.6 60.3\n")
                    f.write("53 53 29845 1198.2 Right-Hippocampus 108.7 14.8 82.1 142.3 60.2\n")
                logger.info(f"aseg_stats_created: {aseg_stats}")
            except Exception as e:
                logger.error(f"aseg_stats_creation_failed: {e} at {aseg_stats}")

            # Create mock segmentation files for visualization
            try:
                self._create_mock_segmentation_files(mri_dir)
                logger.info("mock_segmentation_files_created")
            except Exception as e:
                logger.error(f"mock_segmentation_files_failed: {e}")

            logger.info(f"mock_output_created: {output_dir}")
        except Exception as e:
            logger.error(f"create_mock_fastsurfer_output_failed: {e}")
            raise

    def _create_mock_segmentation_files(self, mri_dir: Path) -> None:
        """
        Create mock segmentation files needed for visualization.

        Args:
            mri_dir: MRI directory to create files in
        """
        import numpy as np
        import nibabel as nib

        logger.info("creating_mock_segmentation_files")

        # Create a simple 3D brain-like volume (64x64x32) for mock data
        shape = (64, 64, 32)
        data = np.zeros(shape, dtype=np.int16)

        # Add mock hippocampus regions
        # Left hippocampus (label 17) - roughly in the temporal lobe area
        data[20:30, 15:25, 10:20] = 17
        # Right hippocampus (label 53)
        data[35:45, 15:25, 10:20] = 53

        # Add some other brain structures for realism
        data[25:35, 25:35, 15:25] = 2  # Left cerebral white matter
        data[30:40, 25:35, 15:25] = 41  # Right cerebral white matter

        # Create affine matrix (simple scaling)
        affine = np.diag([1.0, 1.0, 1.0, 1.0])

        # Create mock T1 anatomical image (orig.mgz)
        t1_data = np.random.normal(1000, 100, size=shape).astype(np.float32)
        # Make hippocampus areas slightly different intensity
        t1_data[20:30, 15:25, 10:20] = np.random.normal(1100, 50, size=(10, 10, 10))
        t1_data[35:45, 15:25, 10:20] = np.random.normal(1100, 50, size=(10, 10, 10))

        # Save orig.mgz (T1 anatomical)
        orig_img = nib.Nifti1Image(t1_data, affine)
        orig_path = mri_dir / "orig.mgz"
        nib.save(orig_img, orig_path)

        # Save aparc.DKTatlas+aseg.deep.mgz (segmentation)
        seg_img = nib.Nifti1Image(data, affine)
        aseg_path = mri_dir / "aparc.DKTatlas+aseg.deep.mgz"
        nib.save(seg_img, aseg_path)

        # Create mock hippocampal subfield files (optional)
        # Left hippocampus subfields
        lh_subfields = np.zeros(shape, dtype=np.int16)
        lh_subfields[20:25, 15:25, 10:20] = 203  # CA1
        lh_subfields[25:30, 15:25, 10:20] = 204  # CA3

        # Right hippocampus subfields
        rh_subfields = np.zeros(shape, dtype=np.int16)
        rh_subfields[35:40, 15:25, 10:20] = 1203  # CA1_right
        rh_subfields[40:45, 15:25, 10:20] = 1204  # CA3_right

        # Save subfield files
        lh_img = nib.Nifti1Image(lh_subfields, affine)
        lh_path = mri_dir / "lh.hippoSfLabels-T1.v21.mgz"
        nib.save(lh_img, lh_path)

        rh_img = nib.Nifti1Image(rh_subfields, affine)
        rh_path = mri_dir / "rh.hippoSfLabels-T1.v21.mgz"
        nib.save(rh_img, rh_path)

        logger.info("mock_segmentation_files_created",
                   orig=str(orig_path),
                   aseg=str(aseg_path),
                   lh_subfields=str(lh_path),
                   rh_subfields=str(rh_path))
    
    def _generate_aseg_stats_from_mgz(self, subject_dir: Path, aseg_mgz_file: Path, output_stats_file: Path) -> None:
        """
        Generate aseg.stats from aseg.mgz using FreeSurfer's mri_segstats.

        Args:
            subject_dir: FreeSurfer subject directory
            aseg_mgz_file: Path to aseg.mgz file
            output_stats_file: Where to save the stats file
        """
        logger.info("generating_aseg_stats_from_mgz",
                   subject_dir=str(subject_dir),
                   aseg_mgz=str(aseg_mgz_file),
                   output=str(output_stats_file))

        # Use the same container and environment as the main FreeSurfer run
        sif_path = self._find_freesurfer_sif()
        if not sif_path:
            raise RuntimeError("FreeSurfer SIF not found for stats generation")

        license_path = self._get_freesurfer_license_path()
        if not license_path:
            raise RuntimeError("FreeSurfer license not found")

        # mri_segstats command to generate statistics
        segstats_cmd = [
            "apptainer", "exec",
            "--bind", f"{subject_dir}:/subjects",
            "--bind", f"{license_path}:/usr/local/freesurfer/license.txt:ro",
            "--env", f"FS_LICENSE=/usr/local/freesurfer/license.txt",
            "--env", f"SUBJECTS_DIR=/subjects",
            str(sif_path),
            "mri_segstats",
            "--seg", "/subjects/mri/aseg.mgz",  # Use absolute path inside container
            "--sum", "/subjects/stats/aseg.stats",
            "--pv", "/subjects/mri/norm.mgz",
            "--empty",
            "--brain-vol",
            "--subject", subject_dir.name
        ]

        logger.info("running_mri_segstats", command=" ".join(segstats_cmd))

        result = subprocess_module.run(
            segstats_cmd,
            capture_output=True,
            timeout=300,  # 5 minutes timeout for stats generation
            text=True,
            env=self._get_extended_env()
        )

        if result.returncode != 0:
            error_msg = result.stderr or result.stdout
            logger.error("mri_segstats_failed",
                        returncode=result.returncode,
                        error=error_msg[:500])
            raise RuntimeError(f"mri_segstats failed: {error_msg[:200]}")

        logger.info("mri_segstats_completed")

    def _extract_hippocampal_data(self, freesurfer_dir: Path) -> Dict:
        """
        Extract hippocampal volumes from FreeSurfer output.

        Handles both FreeSurfer and FastSurfer output formats.
        Tries FreeSurfer aseg.stats first, then FastSurfer aseg+DKT.stats.

        Args:
            freesurfer_dir: FreeSurfer/FastSurfer output directory

        Returns:
            Dictionary of hippocampal volumes by region and hemisphere
        """
        logger.info("extracting_hippocampal_data")

        # Check FreeSurfer structure first (new primary)
        freesurfer_stats_dir = None

        # Look for FreeSurfer subject directories
        for subdir in ["freesurfer_singularity", "freesurfer_docker", "freesurfer_fallback"]:
            # Check both direct path and nested Docker path
            candidate_dirs = [
                freesurfer_dir / f"{subdir}_{self.job_id}" / "stats",  # Direct path
                freesurfer_dir / subdir / f"{subdir}_{self.job_id}" / "stats"  # Nested Docker path
            ]

            for candidate_dir in candidate_dirs:
                if candidate_dir.exists():
                    freesurfer_stats_dir = candidate_dir
                    logger.info("found_freesurfer_stats_directory", stats_dir=str(freesurfer_stats_dir), subdir=subdir)
                    break

            if freesurfer_stats_dir:
                break

        # Fall back to FastSurfer structure
        if not freesurfer_stats_dir:
            fastsurfer_stats_dir = freesurfer_dir / str(self.job_id) / "stats"
            if fastsurfer_stats_dir.exists():
                freesurfer_stats_dir = fastsurfer_stats_dir
                logger.info("found_fastsurfer_stats_directory", stats_dir=str(freesurfer_stats_dir))

        # Fall back to mock data
        mock_stats_dir = freesurfer_dir / f"mock_freesurfer_{self.job_id}" / "stats"
        if not freesurfer_stats_dir and mock_stats_dir.exists():
            freesurfer_stats_dir = mock_stats_dir
            logger.info("using_mock_freesurfer_stats_directory", stats_dir=str(freesurfer_stats_dir))

        if not freesurfer_stats_dir:
            logger.warning("no_stats_directory_found")
            return {}

        # Try FreeSurfer aseg.stats first
        logger.info("checking_for_freesurfer_aseg_data")
        aseg_file = freesurfer_stats_dir / "aseg.stats"

        hippocampal_data = {}

        if aseg_file.exists():
            logger.info("using_freesurfer_aseg_data", file=str(aseg_file))
            volumes = segmentation.parse_aseg_stats(aseg_file)
            if volumes:
                hippocampal_data = {
                    "Hippocampus": {
                        "left": volumes.get("left", 0.0),
                        "right": volumes.get("right", 0.0),
                    }
                }
                logger.info(
                    "freesurfer_hippocampal_data_found",
                    left=volumes.get("left"),
                    right=volumes.get("right")
                )
        else:
            # Try to generate aseg.stats from aseg.mgz if FreeSurfer created segmentation but not stats
            aseg_mgz_file = freesurfer_stats_dir.parent / "mri" / "aseg.mgz"
            if aseg_mgz_file.exists():
                logger.info("aseg.stats_missing_generating_from_mgz", mgz_file=str(aseg_mgz_file))
                try:
                    # Generate stats using mri_segstats (run inside FreeSurfer environment)
                    self._generate_aseg_stats_from_mgz(freesurfer_stats_dir.parent, aseg_mgz_file, aseg_file)
                    if aseg_file.exists():
                        logger.info("aseg_stats_generated_successfully")
                        volumes = segmentation.parse_aseg_stats(aseg_file)
                        if volumes:
                            hippocampal_data = {
                                "Hippocampus": {
                                    "left": volumes.get("left", 0.0),
                                    "right": volumes.get("right", 0.0),
                                }
                            }
                            logger.info(
                                "freesurfer_hippocampal_data_found",
                                left=volumes.get("left"),
                                right=volumes.get("right")
                            )
                    else:
                        logger.warning("failed_to_generate_aseg_stats")
                except Exception as e:
                    logger.error("error_generating_aseg_stats", error=str(e))
            else:
                logger.warning("no_aseg_files_found")

        # If no FreeSurfer data found, try FastSurfer fallback
        if not hippocampal_data:
            # Fall back to FastSurfer aseg+DKT.stats
            # Fall back to FastSurfer aseg+DKT.stats
            logger.info("freesurfer_stats_not_found_trying_fastsurfer")
            fastsurfer_aseg_file = freesurfer_stats_dir / "aseg+DKT.stats"

            if fastsurfer_aseg_file.exists():
                logger.info("using_fastsurfer_aseg_data", file=str(fastsurfer_aseg_file))
                volumes = segmentation.parse_aseg_stats(fastsurfer_aseg_file)
                if volumes:
                    hippocampal_data = {
                        "Hippocampus": {
                            "left": volumes.get("left", 0.0),
                            "right": volumes.get("right", 0.0),
                        }
                    }
                    logger.info(
                        "fastsurfer_hippocampal_data_found",
                        left=volumes.get("left"),
                        right=volumes.get("right")
                    )
                    logger.info(
                        "freesurfer_hippocampal_data_found",
                        left=volumes.get("left_hippocampus"),
                        right=volumes.get("right_hippocampus")
                    )
                else:
                    logger.warning("no_hippocampal_volumes_found_in_freesurfer")
            else:
                logger.error("no_stats_files_found", stats_dir=str(freesurfer_stats_dir), tried_files=["aseg+DKT.stats", "aseg.stats"])
        
        logger.info(
            "hippocampal_data_extracted",
            regions=list(hippocampal_data.keys()),
        )
        
        return hippocampal_data
    
    def _calculate_asymmetry(self, hippocampal_data: Dict) -> List[Dict]:
        """
        Calculate asymmetry indices for each hippocampal region.
        
        Args:
            hippocampal_data: Dictionary of volumes by region
        
        Returns:
            List of metric dictionaries
        """
        logger.info("calculating_asymmetry_indices")
        
        metrics = []
        
        for region, volumes in hippocampal_data.items():
            left = volumes["left"]
            right = volumes["right"]
            
            # Calculate asymmetry index
            ai = asymmetry.calculate_asymmetry_index(left, right)
            
            metrics.append({
                "region": region,
                "left_volume": left,
                "right_volume": right,
                "asymmetry_index": ai,
            })
        
        logger.info("asymmetry_calculated", metrics_count=len(metrics))
        
        return metrics
    
    def _generate_visualizations(self, nifti_path: Path, freesurfer_dir: Path) -> Dict[str, any]:
        """
        Generate segmentation visualizations for web viewer.
        
        Args:
            nifti_path: Path to original T1 NIfTI
            freesurfer_dir: FastSurfer output directory
        
        Returns:
            Dictionary with visualization file paths
        """
        logger.info("generating_visualizations")
        
        viz_dir = self.output_dir / "visualizations"
        viz_dir.mkdir(parents=True, exist_ok=True)
        
        viz_paths = {
            "whole_hippocampus": None,
            "subfields": None,
            "overlays": {}
        }
        
        try:
            # Extract segmentation files from FreeSurfer output
            aseg_nii, subfields_nii = visualization.extract_hippocampus_segmentation(
                freesurfer_dir,
                str(self.job_id)
            )
            
            # Convert anatomical T1 image for viewer base layer
            # FreeSurfer uses T1.mgz (conformed space) to ensure alignment with segmentation
            # Look for FreeSurfer subject directories first
            freesurfer_subject_dir = None
            for subdir in ["freesurfer_singularity", "freesurfer_docker", "freesurfer_fallback"]:
                candidate_dir = freesurfer_dir / f"{subdir}_{self.job_id}"
                if candidate_dir.exists():
                    freesurfer_subject_dir = candidate_dir
                    break

            t1_nifti = None
            if freesurfer_subject_dir:
                # Use FreeSurfer's T1.mgz for proper alignment
                t1_mgz = freesurfer_subject_dir / "mri" / "T1.mgz"
                if t1_mgz.exists():
                    t1_nifti = visualization.convert_t1_to_nifti(
                        t1_mgz,
                        viz_dir / "whole_hippocampus"
                    )
                    logger.info("t1_anatomical_converted", path=str(t1_nifti))
                else:
                    logger.warning("t1_mgz_not_found_in_freesurfer",
                                 expected=str(t1_mgz),
                                 note="Will use original input - may have alignment issues")

            # Fallback to original input if no FreeSurfer subject directory found
            if not t1_nifti:
                logger.warning("freesurfer_subject_dir_not_found",
                             note="Will use original input - may have alignment issues")
                t1_nifti = nifti_path
                
                # Generate overlay images for ALL 3 orientations
                # Use orig.mgz converted T1 to ensure proper spatial alignment with segmentation
                # FreeSurfer labels: 17 = Left-Hippocampus, 53 = Right-Hippocampus
                all_overlays = visualization.generate_all_orientation_overlays(
                    t1_nifti,  # Use orig.mgz converted (in same space as segmentation)
                    aseg_nii,
                    viz_dir / "overlays",
                    prefix="hippocampus",
                    specific_labels=[17, 53]  # Highlight hippocampus only
                )
                viz_paths["overlays"] = all_overlays
            
            # Prepare subfields for viewer
            if subfields_nii and subfields_nii.exists():
                subfields = visualization.prepare_nifti_for_viewer(
                    subfields_nii,
                    viz_dir / "subfields",
                    visualization.HIPPOCAMPAL_SUBFIELD_LABELS
                )
                viz_paths["subfields"] = subfields
                
                # Generate subfield overlay images
                # Use orig.mgz converted T1 to ensure proper spatial alignment
                subfield_overlays = visualization.generate_segmentation_overlays(
                    t1_nifti,  # Use orig.mgz converted (in same space as segmentation)
                    subfields_nii,
                    viz_dir / "overlays",
                    prefix="subfields"
                )
                viz_paths["overlays"]["subfields"] = subfield_overlays
            
            logger.info("visualizations_generated", paths=viz_paths)
        
        except Exception as e:
            logger.error("visualization_generation_failed", error=str(e), exc_info=True)
        
        return viz_paths
    
    def _store_process_pid(self, pid: int) -> None:
        """
        Store the process PID for tracking and cleanup.
        
        Writes PID to a file so we can kill zombie processes later.
        
        Args:
            pid: Process ID to store
        """
        self.process_pid = pid
        pid_file = self.output_dir / ".process_pid"
        with open(pid_file, "w") as f:
            f.write(str(pid))
        logger.info("process_pid_stored", pid=pid, file=str(pid_file))
    
    def _clear_process_pid(self) -> None:
        """Clear stored process PID after completion."""
        self.process_pid = None
        pid_file = self.output_dir / ".process_pid"
        if pid_file.exists():
            pid_file.unlink()
            logger.info("process_pid_cleared")
    
    def _save_results(self, metrics: List[Dict]) -> None:
        """
        Save processing results to files.
        
        Args:
            metrics: List of metric dictionaries
        """
        logger.info("saving_results")
        
        # Save as JSON
        json_path = self.output_dir / "metrics.json"
        with open(json_path, "w") as f:
            json.dump(metrics, f, indent=2)
        
        # Save as CSV
        csv_path = self.output_dir / "metrics.csv"
        df = pd.DataFrame(metrics)
        df.to_csv(csv_path, index=False)
        
        logger.info(
            "results_saved",
            json=str(json_path),
            csv=str(csv_path),
        )

    def _get_app_root_directory(self) -> Path:
        """Get the application root directory.

        Returns the parent directory of the pipeline folder.
        """
        import inspect
        from pathlib import Path
        current_file = Path(inspect.getfile(self.__class__))
        app_root = current_file.parent.parent.parent  # Go up from pipeline/processors/mri_processor.py
        return app_root
    
    def _calculate_asymmetry(self, hippocampal_data: Dict) -> List[Dict]:
        """
        Calculate asymmetry indices for each hippocampal region.
        
        Args:
            hippocampal_data: Dictionary of volumes by region
        
        Returns:
            List of metric dictionaries
        """
        logger.info("calculating_asymmetry_indices")
        
        metrics = []
        
        for region, volumes in hippocampal_data.items():
            left = volumes["left"]
            right = volumes["right"]
            
            # Calculate asymmetry index
            ai = asymmetry.calculate_asymmetry_index(left, right)
            
            metrics.append({
                "region": region,
                "left_volume": left,
                "right_volume": right,
                "asymmetry_index": ai,
            })
        
        logger.info("asymmetry_calculated", metrics_count=len(metrics))
        
        return metrics
    
    def _generate_visualizations(self, nifti_path: Path, freesurfer_dir: Path) -> Dict[str, any]:
        """
        Generate segmentation visualizations for web viewer.
        
        Args:
            nifti_path: Path to original T1 NIfTI
            freesurfer_dir: FastSurfer output directory
        
        Returns:
            Dictionary with visualization file paths
        """
        logger.info("generating_visualizations")
        
        viz_dir = self.output_dir / "visualizations"
        viz_dir.mkdir(parents=True, exist_ok=True)
        
        viz_paths = {
            "whole_hippocampus": None,
            "subfields": None,
            "overlays": {}
        }
        
        try:
            # Extract segmentation files from FreeSurfer output
            aseg_nii, subfields_nii = visualization.extract_hippocampus_segmentation(
                freesurfer_dir,
                str(self.job_id)
            )
            
            # Convert anatomical T1 image for viewer base layer
            # FreeSurfer uses T1.mgz (conformed space) to ensure alignment with segmentation
            # Look for FreeSurfer subject directories first
            freesurfer_subject_dir = None
            for subdir in ["freesurfer_singularity", "freesurfer_docker", "freesurfer_fallback"]:
                candidate_dir = freesurfer_dir / f"{subdir}_{self.job_id}"
                if candidate_dir.exists():
                    freesurfer_subject_dir = candidate_dir
                    break

            t1_nifti = None
            if freesurfer_subject_dir:
                # Use FreeSurfer's T1.mgz for proper alignment
                t1_mgz = freesurfer_subject_dir / "mri" / "T1.mgz"
                if t1_mgz.exists():
                    t1_nifti = visualization.convert_t1_to_nifti(
                        t1_mgz,
                        viz_dir / "whole_hippocampus"
                    )
                    logger.info("t1_anatomical_converted", path=str(t1_nifti))
                else:
                    logger.warning("t1_mgz_not_found_in_freesurfer",
                                 expected=str(t1_mgz),
                                 note="Will use original input - may have alignment issues")

            # Fallback to original input if no FreeSurfer subject directory found
            if not t1_nifti:
                logger.warning("freesurfer_subject_dir_not_found",
                             note="Will use original input - may have alignment issues")
                t1_nifti = nifti_path
            
            # Prepare whole hippocampus for viewer
            # Show whole brain but only highlight hippocampus in legend
            if aseg_nii and aseg_nii.exists():
                whole_hippo = visualization.prepare_nifti_for_viewer(
                    aseg_nii,
                    viz_dir / "whole_hippocampus",
                    visualization.ASEG_HIPPOCAMPUS_LABELS,
                    highlight_labels=[17, 53]  # Only show hippocampus in legend
                )
                viz_paths["whole_hippocampus"] = whole_hippo
                
                # Generate overlay images for ALL 3 orientations
                # Use orig.mgz converted T1 to ensure proper spatial alignment with segmentation
                # FreeSurfer labels: 17 = Left-Hippocampus, 53 = Right-Hippocampus
                all_overlays = visualization.generate_all_orientation_overlays(
                    t1_nifti,  # Use orig.mgz converted (in same space as segmentation)
                    aseg_nii,
                    viz_dir / "overlays",
                    prefix="hippocampus",
                    specific_labels=[17, 53]  # Highlight hippocampus only
                )
                viz_paths["overlays"] = all_overlays
            
            # Prepare subfields for viewer
            if subfields_nii and subfields_nii.exists():
                subfields = visualization.prepare_nifti_for_viewer(
                    subfields_nii,
                    viz_dir / "subfields",
                    visualization.HIPPOCAMPAL_SUBFIELD_LABELS
                )
                viz_paths["subfields"] = subfields
                
                # Generate subfield overlay images
                # Use orig.mgz converted T1 to ensure proper spatial alignment
                subfield_overlays = visualization.generate_segmentation_overlays(
                    t1_nifti,  # Use orig.mgz converted (in same space as segmentation)
                    subfields_nii,
                    viz_dir / "overlays",
                    prefix="subfields"
                )
                viz_paths["overlays"]["subfields"] = subfield_overlays
            
            logger.info("visualizations_generated", paths=viz_paths)
        
        except Exception as e:
            logger.error("visualization_generation_failed", error=str(e), exc_info=True)
        
        return viz_paths
    
    def _store_process_pid(self, pid: int) -> None:
        """
        Store the process PID for tracking and cleanup.
        
        Writes PID to a file so we can kill zombie processes later.
        
        Args:
            pid: Process ID to store
        """
        self.process_pid = pid
        pid_file = self.output_dir / ".process_pid"
        with open(pid_file, "w") as f:
            f.write(str(pid))
        logger.info("process_pid_stored", pid=pid, file=str(pid_file))
    
    def _clear_process_pid(self) -> None:
        """Clear stored process PID after completion."""
        self.process_pid = None
        pid_file = self.output_dir / ".process_pid"
        if pid_file.exists():
            pid_file.unlink()
            logger.info("process_pid_cleared")
    
    def _save_results(self, metrics: List[Dict]) -> None:
        """
        Save processing results to files.
        
        Args:
            metrics: List of metric dictionaries
        """
        logger.info("saving_results")
        
        # Save as JSON
        json_path = self.output_dir / "metrics.json"
        with open(json_path, "w") as f:
            json.dump(metrics, f, indent=2)
        
        # Save as CSV
        csv_path = self.output_dir / "metrics.csv"
        df = pd.DataFrame(metrics)
        df.to_csv(csv_path, index=False)
        
        logger.info(
            "results_saved",
            json=str(json_path),
            csv=str(csv_path),
        )

    def _get_app_root_directory(self) -> Path:
        """Get the application root directory.

        Returns the parent directory of the pipeline folder.
        """
        import inspect
        from pathlib import Path
        current_file = Path(inspect.getfile(self.__class__))
        app_root = current_file.parent.parent.parent  # Go up from pipeline/processors/mri_processor.py
        return app_root

    def _start_freesurfer_progress_monitor(self, status_log_path: Path, base_progress: int = 20) -> None:
        """Start monitoring FreeSurfer progress by parsing recon-all-status.log.

        This runs in a separate thread and updates progress based on FreeSurfer's
        completion of different processing stages.

        Args:
            status_log_path: Path to the recon-all-status.log file
            base_progress: Base progress percentage (default 20 for FreeSurfer start)
        """
        import threading
        import time

        def monitor_progress():
            """Monitor the status log file and update progress."""
            last_line_count = 0
            last_detected_phase = None
            phase_progress_map = {
                # Map actual FreeSurfer phases to progress percentages
                # Based on actual recon-all-status.log entries
                "motioncor": base_progress + 5,
                "talairach": base_progress + 10,
                "nu intensity correction": base_progress + 15,
                "intensity normalization": base_progress + 20,
                "skull stripping": base_progress + 25,
                "em registration": base_progress + 35,
                "ca normalize": base_progress + 45,
                "ca reg": base_progress + 55,
                "subcort seg": base_progress + 65,
                "wm segmentation": base_progress + 75,
                "fill": base_progress + 85,
                "cc seg": base_progress + 90,
                "finished": 100
            }
            
            logger.info("freesurfer_progress_monitor_thread_started", 
                       log_path=str(status_log_path), 
                       base_progress=base_progress,
                       job_id=str(self.job_id))

            # Wait for status log to be created (max 5 minutes)
            wait_count = 0
            while not status_log_path.exists() and wait_count < 10:
                logger.debug("waiting_for_status_log", path=str(status_log_path), wait_count=wait_count)
                time.sleep(30)
                wait_count += 1
            
            if not status_log_path.exists():
                logger.warning("status_log_never_created", path=str(status_log_path))
                return

            logger.info("status_log_found", path=str(status_log_path))

            while True:
                try:
                    if status_log_path.exists():
                        with open(status_log_path, 'r') as f:
                            lines = f.readlines()

                        if len(lines) > last_line_count:
                            # New lines detected
                            new_lines = lines[last_line_count:]
                            last_line_count = len(lines)
                            
                            logger.debug("freesurfer_log_new_lines", count=len(new_lines), total_lines=len(lines))

                            for line in new_lines:
                                original_line = line.strip()
                                line_lower = original_line.lower()
                                
                                # Check for FreeSurfer status markers: #@# PhaseName
                                if line_lower.startswith("#@#"):
                                    logger.info("freesurfer_log_line_found", line=original_line, job_id=str(self.job_id))
                                    
                                    # Try to match with known phases
                                    matched = False
                                    for phase, progress in phase_progress_map.items():
                                        if phase in line_lower:
                                            if last_detected_phase != phase:  # Only update if it's a new phase
                                                self._update_progress(progress, f"FreeSurfer: {phase.title()} completed")
                                            logger.info("freesurfer_phase_completed",
                                                           phase=phase,
                                                           progress=progress,
                                                           line=original_line,
                                                           job_id=str(self.job_id))
                                            last_detected_phase = phase
                                            matched = True
                                            break
                                    
                                    if not matched:
                                        logger.debug("freesurfer_phase_not_matched", line=original_line)
                    else:
                        # Status log no longer exists, processing might be complete
                        logger.info("status_log_disappeared", path=str(status_log_path))
                        break

                    # Wait before checking again
                    time.sleep(10)  # Check every 10 seconds (more frequent than before)

                except Exception as e:
                    logger.error("freesurfer_progress_monitor_error", 
                                error=str(e), 
                                error_type=type(e).__name__,
                                job_id=str(self.job_id))
                    import traceback
                    logger.error("freesurfer_monitor_traceback", traceback=traceback.format_exc())
                    time.sleep(60)  # Wait longer on error
                    continue
            
            logger.info("freesurfer_progress_monitor_thread_ended", job_id=str(self.job_id))

        # Start monitoring in a background thread
        monitor_thread = threading.Thread(target=monitor_progress, daemon=True, name="FreeSurferProgressMonitor")
        monitor_thread.start()
        logger.info("freesurfer_progress_monitor_started", log_path=str(status_log_path), thread_name=monitor_thread.name)

    def _api_bridge_process(self, input_path: str) -> Dict:
        """
        Process MRI using the FreeSurfer API Bridge.

        This method delegates the actual FreeSurfer processing to the API bridge service,
        which manages Docker containers and provides clean HTTP APIs.

        Args:
            input_path: Path to input MRI file

        Returns:
            Dictionary containing processing results
        """
        logger.info("api_bridge_processing_started", job_id=str(self.job_id), input_path=input_path)

        # Get API bridge configuration
        api_bridge_url = os.getenv("API_BRIDGE_URL", "http://localhost:8080")

        # Prepare input file path (ensure it's accessible to API bridge)
        input_path_obj = Path(input_path)
        if not input_path_obj.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")

        # Create output directory for this job
        output_dir = Path(settings.output_dir) / str(self.job_id)
        output_dir.mkdir(parents=True, exist_ok=True)

        # Prepare API request
        process_url = f"{api_bridge_url}/freesurfer/process"
        request_data = {
            "job_id": str(self.job_id),
            "input_file": str(input_path_obj),
            "output_dir": str(output_dir),
            "subject_id": f"subject_{self.job_id}"
        }

        try:
            # Submit processing job to API bridge
            if self.progress_callback:
                self.progress_callback(10, "Submitting job to FreeSurfer API Bridge")

            logger.info("submitting_to_api_bridge", url=process_url, job_id=str(self.job_id))
            response = requests.post(process_url, json=request_data, timeout=30)

            if response.status_code != 200:
                raise Exception(f"API bridge request failed: {response.status_code} - {response.text}")

            response_data = response.json()
            logger.info("api_bridge_job_submitted", response=response_data)

            # Monitor job progress
            status_url = f"{api_bridge_url}/freesurfer/status/{self.job_id}"
            results_url = f"{api_bridge_url}/freesurfer/results/{self.job_id}"

            # Poll for completion (with timeout)
            max_wait_time = 3600  # 1 hour maximum
            poll_interval = 10  # Check every 10 seconds
            elapsed_time = 0

            while elapsed_time < max_wait_time:
                try:
                    # Check status
                    status_response = requests.get(status_url, timeout=10)
                    if status_response.status_code == 200:
                        status_data = status_response.json()

                        # Update progress callback
                        if self.progress_callback and status_data.get("progress"):
                            progress = int(status_data["progress"])
                            message = status_data.get("message", "Processing...")
                            self.progress_callback(progress, message)

                        # Check if completed
                        if status_data.get("status") == "completed":
                            logger.info("api_bridge_job_completed", job_id=str(self.job_id))

                            # Get final results
                            results_response = requests.get(results_url, timeout=10)
                            if results_response.status_code == 200:
                                results_data = results_response.json()
                                logger.info("api_bridge_results_retrieved", job_id=str(self.job_id))

                                # Return standardized results format
                                return {
                                    "status": "completed",
                                    "output_dir": str(output_dir),
                                    "processing_method": "api_bridge",
                                    "api_bridge_results": results_data.get("results", {}),
                                    "job_id": str(self.job_id)
                                }
                            else:
                                raise Exception(f"Failed to get results: {results_response.status_code}")

                        elif status_data.get("status") == "failed":
                            error_msg = status_data.get("error", "Unknown error from API bridge")
                            raise Exception(f"FreeSurfer processing failed: {error_msg}")

                    elif status_response.status_code == 404:
                        # Job not found, might still be starting
                        logger.debug("job_not_found_waiting", job_id=str(self.job_id))

                except requests.RequestException as e:
                    logger.warning(f"Status check failed: {e}")

                # Wait before next poll
                time.sleep(poll_interval)
                elapsed_time += poll_interval

                # Update progress periodically
                if self.progress_callback and elapsed_time % 60 == 0:  # Every minute
                    minutes_elapsed = elapsed_time // 60
                    self.progress_callback(
                        min(90, 20 + minutes_elapsed),  # Progress from 20% to 90%
                        f"Processing with FreeSurfer... ({minutes_elapsed}min elapsed)"
                    )

            # Timeout reached
            raise Exception(f"FreeSurfer processing timed out after {max_wait_time} seconds")

        except Exception as e:
            logger.error("api_bridge_processing_failed", job_id=str(self.job_id), error=str(e))
            raise Exception(f"API bridge processing failed: {str(e)}")

    def _mock_process(self, input_path: str) -> Dict:
        """
        Mock MRI processing for testing without FreeSurfer license.

        Creates simulated processing results and output files to test
        the complete pipeline workflow.
        """
        import time
        logger.info("mock_processing_started", job_id=str(self.job_id), input_path=input_path)

        # Create output directory
        output_dir = Path(settings.output_dir) / str(self.job_id)
        output_dir.mkdir(parents=True, exist_ok=True)

        # Simulate processing steps with progress updates
        steps = [
            (10, "Initializing mock processor"),
            (20, "Loading input file"),
            (40, "Running mock segmentation"),
            (60, "Extracting mock metrics"),
            (80, "Generating visualizations"),
            (95, "Finalizing results")
        ]

        for progress, step in steps:
            if self.progress_callback:
                self.progress_callback(progress, step)
            time.sleep(0.5)  # Simulate processing time

        # Create mock output files
        mock_files = [
            "aseg.stats",
            "lh.hippoSfVolumes-T1.v21.txt",
            "rh.hippoSfVolumes-T1.v21.txt",
            "brain.nii.gz",
            "aseg.nii.gz"
        ]

        for filename in mock_files:
            file_path = output_dir / filename
            if filename.endswith('.txt'):
                # Create mock stats file
                with open(file_path, 'w') as f:
                    f.write("# Mock FreeSurfer Statistics\n")
                    f.write("# Generated for testing purposes\n")
                    f.write(f"# Job ID: {self.job_id}\n")
                    f.write("# Left Hippocampus: 4200 mm¬≥\n")
                    f.write("# Right Hippocampus: 4100 mm¬≥\n")
            elif filename.endswith('.nii.gz'):
                # Create a minimal mock NIfTI file (just header, no actual data)
                import nibabel as nib
                import numpy as np
                data = np.zeros((64, 64, 32), dtype=np.uint8)  # Minimal 3D volume
                affine = np.eye(4)  # Identity matrix
                img = nib.Nifti1Image(data, affine)
                nib.save(img, str(file_path))
            else:
                # Create empty file for other types
                file_path.touch()

        # Generate mock hippocampal data (structured as expected by _calculate_asymmetry)
        hippocampal_data = {
            "CA1": {"left": 850.2, "right": 840.5},
            "CA2": {"left": 720.8, "right": 710.2},
            "CA3": {"left": 680.5, "right": 670.8},
            "DG": {"left": 450.1, "right": 440.3},
            "SUB": {"left": 320.4, "right": 310.6},
            "ERC": {"left": 890.6, "right": 880.9},
            "PRC": {"left": 950.2, "right": 940.5},
            "PHC": {"left": 850.1, "right": 840.4}
        }

        # Calculate mock asymmetry
        asymmetry_metrics = self._calculate_asymmetry(hippocampal_data)

        # Generate mock visualizations
        visualizations = {
            "hippocampal_volumes": "/api/visualizations/mock/hippocampal_volumes.png",
            "asymmetry_plot": "/api/visualizations/mock/asymmetry_plot.png",
            "segmentation_overlay": "/api/visualizations/mock/segmentation_overlay.png"
        }

        # Create results summary
        results = {
            "output_dir": str(output_dir),
            "hippocampal_data": hippocampal_data,
            "asymmetry_metrics": asymmetry_metrics,
            "visualizations": visualizations,
            "processing_time_seconds": 45.2,
            "mock_processing": True,
            "note": "This is mock data generated for testing without FreeSurfer license"
        }

        logger.info("mock_processing_completed", job_id=str(self.job_id), output_dir=str(output_dir))

        # Ensure any orphaned containers for this job are cleaned up
        self._cleanup_job_containers()

        return results

